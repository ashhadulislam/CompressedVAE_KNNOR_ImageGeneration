{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models import *\n",
    "from experiment import VAEXperiment\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# # Plot some training images\n",
    "# real_batch = next(iter(train_loader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "model_nm=\"VanillaVAE\"\n",
    "args_filename=\"configs/vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "        \n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "chk_path=\"logs/\"+model_nm+\"/version_2/checkpoints/last.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "        \n",
    "    \n",
    "data = VAEDataset(**config[\"data_params\"])\n",
    "data.setup()\n",
    "tloader=data.test_dataloader()\n",
    "train_loader=data.train_dataloader()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.02.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(train_loader))\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(tloader))\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_path=\"models/GANs/netG1.p\"\n",
    "netG.load_state_dict(torch.load(gen_path,map_location=device))\n",
    "\n",
    "disc_path=\"models/GANs/netD1.p\"\n",
    "netD.load_state_dict(torch.load(disc_path,map_location=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us check the discriminators performance on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D_losses = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        real_cpu = data[0].to(device)\n",
    "#         print(real_cpu.shape)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)        \n",
    "#         print(errD_real)\n",
    "        D_losses.append(errD_real)\n",
    "        if i>8:\n",
    "            break\n",
    "print(\"Average loss over training = \",np.mean(D_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D_losses = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tloader, 0):\n",
    "        real_cpu = data[0].to(device)\n",
    "#         print(real_cpu.shape)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)        \n",
    "#         print(errD_real)\n",
    "        D_losses.append(errD_real)\n",
    "        if i>8:\n",
    "            break\n",
    "\n",
    "print(\"Average loss over testing real data= \",np.mean(D_losses))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output, label)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on artificial data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the standalone Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_model = models.alexnet(pretrained=True)\n",
    "# Here the size of each output sample is set to 2.\n",
    "alex_model.classifier[6] = nn.Linear(4096,2)\n",
    "alex_model = alex_model.to(device)\n",
    "PATH=\"../../AFaceDetector/models/s1.pt\"\n",
    "alex_model.load_state_dict(torch.load(PATH))\n",
    "alex_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test some gan data on alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error or loss is  tensor(1.2197, grad_fn=<NllLossBackward>)\n",
      "Accuracy is  tensor(0.2440)\n"
     ]
    }
   ],
   "source": [
    "b_size=2000\n",
    "noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "# Generate fake image batch with G\n",
    "fake = netG(noise)\n",
    "\n",
    "# normalize the values\n",
    "fake=(fake-torch.min(fake))/(torch.max(fake)-torch.min(fake))\n",
    "\n",
    "labels = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "# Classify all fake batch with D\n",
    "outputs = alex_model(fake)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "_,preds=torch.max(outputs,1)\n",
    "loss=criterion(outputs,labels)\n",
    "\n",
    "# Calculate D's loss on the all-fake batch\n",
    "errD_fake =  loss\n",
    "\n",
    "print(\"Error or loss is \",errD_fake)\n",
    "running_corrects = torch.sum(preds == labels.data)\n",
    "acc=running_corrects/labels.shape[0]\n",
    "print(\"Accuracy is \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 488.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        1512.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAARsUlEQVR4nO3de4xc51nH8e8Pm4aWEprgTWRsg11kCk5ERbuYcFUhoLgtqoNEJJdLrBLJogQoCERjkMgfyFIqEJcKUmSloa6oYlmlEHNJITKUgEgaNr0ljjFZarCXmHhLuZQiBew+/DEHadiMs7Mzs7Ndv9+PtJpznvOeOc+rtX5zfGbmbKoKSVIbvmCtG5AkTY+hL0kNMfQlqSGGviQ1xNCXpIZsXOsGlrNp06bavn37WrchSevKE0888amqmlla/7wP/e3btzM3N7fWbUjSupLkHwfVvbwjSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kN+bz/Rq4kraXtd/3Rmhz3H+5546o8r2f6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ5YN/ST3J7mQ5KkB234mSSXZ1Fc7mGQ+yekkt/TVX5vkyW7bO5NkctOQJA1jmDP99wB7lhaTbAO+GzjbV9sF7ANu6Pa5N8mGbvO7gAPAzu7nBc8pSVpdy4Z+VT0CfHrApl8Ffhaovtpe4GhVPV9VZ4B5YHeSzcDVVfVoVRXwXuDWcZuXJK3MSNf0k7wJ+Keq+viSTVuAc33rC11tS7e8tH655z+QZC7J3OLi4igtSpIGWHHoJ3kZ8PPALwzaPKBWL1IfqKoOV9VsVc3OzMystEVJ0mWM8kdUvgrYAXy8ey92K/CRJLvpncFv6xu7FXi2q28dUJckTdGKz/Sr6smquq6qtlfVdnqB/pqq+mfgOLAvyVVJdtB7w/bxqjoPfCbJTd2ndm4HHpzcNCRJwxjmI5sPAI8Cr0qykOSOy42tqpPAMeBp4IPAnVV1qdv8VuA+em/u/j3w0Ji9S5JWaNnLO1X15mW2b1+yfgg4NGDcHHDjCvuTJE2Q38iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktSQYf5G7v1JLiR5qq/2S0n+Nsknkvxeklf0bTuYZD7J6SS39NVfm+TJbts7uz+QLkmaomHO9N8D7FlSexi4saq+Dvg74CBAkl3APuCGbp97k2zo9nkXcADY2f0sfU5J0ipbNvSr6hHg00tqf1pVF7vVx4Ct3fJe4GhVPV9VZ4B5YHeSzcDVVfVoVRXwXuDWCc1BkjSkSVzT/2HgoW55C3Cub9tCV9vSLS+tD5TkQJK5JHOLi4sTaFGSBGOGfpKfBy4C7/u/0oBh9SL1garqcFXNVtXszMzMOC1KkvpsHHXHJPuB7wFu7i7ZQO8MflvfsK3As11964C6JGmKRjrTT7IHeDvwpqr6r75Nx4F9Sa5KsoPeG7aPV9V54DNJbuo+tXM78OCYvUuSVmjZM/0kDwCvAzYlWQDupvdpnauAh7tPXj5WVT9SVSeTHAOepnfZ586qutQ91VvpfRLopfTeA3gISdJULRv6VfXmAeV3v8j4Q8ChAfU54MYVdSdJmii/kStJDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZNnQT3J/kgtJnuqrXZvk4STPdI/X9G07mGQ+yekkt/TVX5vkyW7bO9P9RXVJ0vQMc6b/HmDPktpdwImq2gmc6NZJsgvYB9zQ7XNvkg3dPu8CDgA7u5+lzylJWmXLhn5VPQJ8ekl5L3CkWz4C3NpXP1pVz1fVGWAe2J1kM3B1VT1aVQW8t28fSdKUjHpN//qqOg/QPV7X1bcA5/rGLXS1Ld3y0vpASQ4kmUsyt7i4OGKLkqSlJv1G7qDr9PUi9YGq6nBVzVbV7MzMzMSak6TWjRr6z3WXbOgeL3T1BWBb37itwLNdfeuAuiRpikYN/ePA/m55P/BgX31fkquS7KD3hu3j3SWgzyS5qfvUzu19+0iSpmTjcgOSPAC8DtiUZAG4G7gHOJbkDuAscBtAVZ1Mcgx4GrgI3FlVl7qneiu9TwK9FHio+5EkTdGyoV9Vb77MppsvM/4QcGhAfQ64cUXdSZImym/kSlJDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyFihn+SnkpxM8lSSB5J8UZJrkzyc5Jnu8Zq+8QeTzCc5neSW8duXJK3EyKGfZAvwE8BsVd0IbAD2AXcBJ6pqJ3CiWyfJrm77DcAe4N4kG8ZrX5K0EuNe3tkIvDTJRuBlwLPAXuBIt/0IcGu3vBc4WlXPV9UZYB7YPebxJUkrMHLoV9U/Ab8MnAXOA/9eVX8KXF9V57sx54Hrul22AOf6nmKhq71AkgNJ5pLMLS4ujtqiJGmJcS7vXEPv7H0H8OXAFyf5wRfbZUCtBg2sqsNVNVtVszMzM6O2KElaYpzLO98FnKmqxar6H+ADwDcDzyXZDNA9XujGLwDb+vbfSu9ykCRpSsYJ/bPATUleliTAzcAp4DiwvxuzH3iwWz4O7EtyVZIdwE7g8TGOL0laoY2j7lhVH07yfuAjwEXgo8Bh4OXAsSR30HthuK0bfzLJMeDpbvydVXVpzP4lSSswcugDVNXdwN1Lys/TO+sfNP4QcGicY0qSRuc3ciWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWSs0E/yiiTvT/K3SU4l+aYk1yZ5OMkz3eM1feMPJplPcjrJLeO3L0laiXHP9H8d+GBVfQ3wauAUcBdwoqp2Aie6dZLsAvYBNwB7gHuTbBjz+JKkFRg59JNcDXw78G6Aqvrvqvo3YC9wpBt2BLi1W94LHK2q56vqDDAP7B71+JKklRvnTP+VwCLw20k+muS+JF8MXF9V5wG6x+u68VuAc337L3Q1SdKUjBP6G4HXAO+qqq8HPkt3KecyMqBWAwcmB5LMJZlbXFwco0VJUr9xQn8BWKiqD3fr76f3IvBcks0A3eOFvvHb+vbfCjw76Imr6nBVzVbV7MzMzBgtSpL6jRz6VfXPwLkkr+pKNwNPA8eB/V1tP/Bgt3wc2JfkqiQ7gJ3A46MeX5K0chvH3P/HgfcleQnwSeAt9F5IjiW5AzgL3AZQVSeTHKP3wnARuLOqLo15fEnSCowV+lX1MWB2wKabLzP+EHBonGNKkkbnN3IlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWTce+98Xtt+1x+tyXH/4Z43rslxJWk5nulLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGjJ26CfZkOSjSf6wW782ycNJnuker+kbezDJfJLTSW4Z99iSpJWZxJn+24BTfet3ASeqaidwolsnyS5gH3ADsAe4N8mGCRxfkjSksUI/yVbgjcB9feW9wJFu+Qhwa1/9aFU9X1VngHlg9zjHlyStzLhn+r8G/Czwub7a9VV1HqB7vK6rbwHO9Y1b6GovkORAkrkkc4uLi2O2KEn6PyOHfpLvAS5U1RPD7jKgVoMGVtXhqpqtqtmZmZlRW5QkLTHOXTa/BXhTkjcAXwRcneR3gOeSbK6q80k2Axe68QvAtr79twLPjnF8SdIKjXymX1UHq2prVW2n9wbtn1XVDwLHgf3dsP3Ag93ycWBfkquS7AB2Ao+P3LkkacVW43769wDHktwBnAVuA6iqk0mOAU8DF4E7q+rSKhxfknQZEwn9qvoQ8KFu+V+Amy8z7hBwaBLHlCStnN/IlaSGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkJFDP8m2JH+e5FSSk0ne1tWvTfJwkme6x2v69jmYZD7J6SS3TGICkqThjXOmfxH46ar6WuAm4M4ku4C7gBNVtRM40a3TbdsH3ADsAe5NsmGc5iVJKzNy6FfV+ar6SLf8GeAUsAXYCxzphh0Bbu2W9wJHq+r5qjoDzAO7Rz2+JGnlJnJNP8l24OuBDwPXV9V56L0wANd1w7YA5/p2W+hqg57vQJK5JHOLi4uTaFGSxARCP8nLgd8FfrKq/uPFhg6o1aCBVXW4qmaranZmZmbcFiVJnbFCP8kX0gv891XVB7ryc0k2d9s3Axe6+gKwrW/3rcCz4xxfkrQy43x6J8C7gVNV9St9m44D+7vl/cCDffV9Sa5KsgPYCTw+6vElSSu3cYx9vwX4IeDJJB/raj8H3AMcS3IHcBa4DaCqTiY5BjxN75M/d1bVpTGOL0laoZFDv6r+isHX6QFuvsw+h4BDox5TkjQev5ErSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWTqoZ9kT5LTSeaT3DXt40tSy6Ya+kk2AL8JvB7YBbw5ya5p9iBJLZv2mf5uYL6qPllV/w0cBfZOuQdJatbGKR9vC3Cub30B+Malg5IcAA50q/+Z5PSIx9sEfGrEfUeWd0z7iP/Pmsx5jTnnK19r8yXvGHvOXzmoOO3Qz4BavaBQdRg4PPbBkrmqmh33edYT59yG1ubc2nxh9eY87cs7C8C2vvWtwLNT7kGSmjXt0P8bYGeSHUleAuwDjk+5B0lq1lQv71TVxSQ/BvwJsAG4v6pOruIhx75EtA455za0NufW5gurNOdUveCSuiTpCuU3ciWpIYa+JDXkigj95W7tkJ53dts/keQ1a9HnpAwx3x/o5vmJJH+d5NVr0eckDXv7jiTfkORSku+bZn+rYZg5J3ldko8lOZnkL6bd46QN8W/7S5P8QZKPd3N+y1r0OSlJ7k9yIclTl9k++eyqqnX9Q+8N4b8HXgm8BPg4sGvJmDcAD9H7nsBNwIfXuu9Vnu83A9d0y69fz/Mdds594/4M+GPg+9a67yn8nl8BPA18Rbd+3Vr3PYU5/xzwjm55Bvg08JK17n2MOX878Brgqctsn3h2XQln+sPc2mEv8N7qeQx4RZLN0250Qpadb1X9dVX9a7f6GL3vQ6xnw96+48eB3wUuTLO5VTLMnL8f+EBVnQWoqvU+72HmXMCXJAnwcnqhf3G6bU5OVT1Cbw6XM/HsuhJCf9CtHbaMMGa9WOlc7qB3prCeLTvnJFuA7wV+a4p9raZhfs9fDVyT5ENJnkhy+9S6Wx3DzPk3gK+l96XOJ4G3VdXnptPemph4dk37NgyrYZhbOwx1+4d1Yui5JPkOeqH/rava0eobZs6/Bry9qi71TgLXvWHmvBF4LXAz8FLg0SSPVdXfrXZzq2SYOd8CfAz4TuCrgIeT/GVV/ccq97ZWJp5dV0LoD3Nrhyvp9g9DzSXJ1wH3Aa+vqn+ZUm+rZZg5zwJHu8DfBLwhycWq+v2pdDh5w/67/lRVfRb4bJJHgFcD6zX0h5nzW4B7qnfBez7JGeBrgMen0+LUTTy7roTLO8Pc2uE4cHv3TvhNwL9X1flpNzohy843yVcAHwB+aB2f9fVbds5VtaOqtlfVduD9wI+u48CH4f5dPwh8W5KNSV5G7461p6bc5yQNM+ez9P5nQ5LrgVcBn5xql9M18exa92f6dZlbOyT5kW77b9H7NMcbgHngv+idLaxLQ873F4AvA+7tznwv1jq+Q+GQc76iDDPnqjqV5IPAJ4DPAfdV1cCP/q0HQ/6efxF4T5In6V36eHtVrdtbLid5AHgdsCnJAnA38IWwetnlbRgkqSFXwuUdSdKQDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkP8FQTUAhXmtaVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(preds.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genenrate GAN artificial images and save them to compar with original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size=2000\n",
    "noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "# Generate fake image batch with G\n",
    "fake = netG(noise)\n",
    "\n",
    "# normalize the values\n",
    "fake=(fake-torch.min(fake))/(torch.max(fake)-torch.min(fake))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "counter=0\n",
    "\n",
    "if not os.path.isdir(\"GANops/all_imgs/\"):\n",
    "    os.mkdir(\"GANops/all_imgs/\")\n",
    "\n",
    "print(\"Saving GAN images\")\n",
    "    \n",
    "for img in fake:\n",
    "    loc=\"GANops/all_imgs/\"\n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.permute(1,2,0)\n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.detach().numpy()\n",
    "#     print(\"shp is \",img.shape,np.min(img),np.max(img))  \n",
    "    if np.min(img)<0 or np.max(img):\n",
    "        img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
    "    \n",
    "    plt.imsave(loc+\"img\"+str(counter)+\".jpeg\",img)\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "    counter+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### artificaial data generated by generator from noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size=1000\n",
    "noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "# Generate fake image batch with G\n",
    "fake = netG(noise)\n",
    "\n",
    "# normalize the values\n",
    "fake=(fake-torch.min(fake))/(torch.max(fake)-torch.min(fake))\n",
    "\n",
    "label = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
    "# Classify all fake batch with D\n",
    "output = netD(fake.detach()).view(-1)\n",
    "# Calculate D's loss on the all-fake batch\n",
    "errD_fake =  criterion(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fake images shape is \",fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(output),torch.max(output),output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on data generated by VAE using KNNOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us use the Decoders one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE - Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm=\"VanillaVAE\"\n",
    "args_filename=\"configs/vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "chk_path=\"logs/\"+model_nm+\"/version_2/checkpoints/last.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "\n",
    "\n",
    "for nm,params in model.named_parameters():\n",
    "#     print(nm)\n",
    "#     print(\"model.\"+nm in checkpoint[\"state_dict\"])\n",
    "    keyy=\"model.\"+nm \n",
    "    params.data=checkpoint[\"state_dict\"][keyy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n"
     ]
    }
   ],
   "source": [
    "X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/test_aug_enc.npy\")\n",
    "mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:mid]\n",
    "    log_var=X_vals_enc_arr[:,mid:]\n",
    "\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    print(mu.shape,log_var.shape)\n",
    "    z = model.reparameterize(mu, log_var)    \n",
    "    images=model.decode(z)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape is  torch.Size([2000, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape is \",images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the values\n",
    "images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "\n",
    "b_size=images.shape[0]\n",
    "\n",
    "\n",
    "label = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
    "# Classify all fake batch with D\n",
    "output = netD(images.detach()).view(-1)\n",
    "# Calculate D's loss on the all-fake batch\n",
    "# errD_fake =  criterion(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 2])\n",
      "torch.Size([2000])\n",
      "Error or loss is  tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "AlexNet Accuracy is  tensor(0.0560)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = alex_model(images)\n",
    "label = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "print(outputs.shape)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(label.shape)\n",
    "_,preds=torch.max(outputs,1)\n",
    "loss=criterion(outputs,label)\n",
    "\n",
    "# Calculate D's loss on the all-fake batch\n",
    "errD_fake =  loss\n",
    "\n",
    "print(\"Error or loss is \",errD_fake)\n",
    "running_corrects = torch.sum(preds == label.data)\n",
    "acc=running_corrects/labels.shape[0]\n",
    "print(\"AlexNet Accuracy is \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4929, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(errD_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 112.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        1888.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(preds.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE - Vanilla Pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm=\"VanillaVAE\"\n",
    "args_filename=\"configs/vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "chk_path=\"logs/\"+model_nm+\"/version_2/checkpoints/last.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "\n",
    "\n",
    "for nm,params in model.named_parameters():\n",
    "#     print(nm)\n",
    "#     print(\"model.\"+nm in checkpoint[\"state_dict\"])\n",
    "    keyy=\"model.\"+nm \n",
    "    params.data=checkpoint[\"state_dict\"][keyy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/VanillaVAE/version_2/checkpoints/last.ckpt\n",
      "logs/VanillaVAE/version_2/checkpoints/epoch=2-step=7631.ckpt\n",
      "Generating mask\n",
      "compression is  0.3886396275937206\n"
     ]
    }
   ],
   "source": [
    "state_dicts=[]\n",
    "epoch_names=[\"last.ckpt\",\"epoch=2-step=7631.ckpt\"]\n",
    "\n",
    "for epoch_name in epoch_names:\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_2/checkpoints/\"+epoch_name\n",
    "    print(chk_path)\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    state_dict=checkpoint[\"state_dict\"]\n",
    "    state_dicts.append(state_dict)\n",
    "    \n",
    "importance_vector=[0.8,0.2]\n",
    "evol_wts={}\n",
    "for nm,params in model.named_parameters():\n",
    "    if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "#         print(nm,params.shape)\n",
    "        keyy=\"model.\"+nm         \n",
    "#         print(state_dicts[0][keyy].shape,state_dicts[1][keyy].shape)\n",
    "#         print(state_dicts[0][keyy][0],state_dicts[1][keyy][0])        \n",
    "        new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "#         print(new_param_values[0])\n",
    "        evol_wts[nm]=new_param_values\n",
    "    \n",
    "prune_rate=0.4\n",
    "print(\"Generating mask\")\n",
    "list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "\n",
    "\n",
    "\n",
    "model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "total_size,nz_size=lib_prune.model_size(model)\n",
    "compression=(total_size-nz_size)/total_size\n",
    "print(\"compression is \",compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n"
     ]
    }
   ],
   "source": [
    "X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/pruned_\"+str(prune_rate)+\"_test_aug_enc.npy\")\n",
    "\n",
    "mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:mid]\n",
    "    log_var=X_vals_enc_arr[:,mid:]\n",
    "\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    print(mu.shape,log_var.shape)\n",
    "    z = model.reparameterize(mu, log_var)    \n",
    "    images=model.decode(z)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape is  torch.Size([2000, 3, 64, 64])\n",
      "torch.Size([2000, 2])\n",
      "torch.Size([2000])\n",
      "Error or loss is  tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "AlexNet Accuracy on prune rate 0.4 compression 0.3886396275937206  is  tensor(0.0560)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape is \",images.shape)\n",
    "\n",
    "# normalize the values\n",
    "images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "\n",
    "b_size=images.shape[0]\n",
    "\n",
    "\n",
    "# Classify all fake batch with D\n",
    "label = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "print(outputs.shape)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(label.shape)\n",
    "_,preds=torch.max(outputs,1)\n",
    "loss=criterion(outputs,label)\n",
    "\n",
    "# Calculate D's loss on the all-fake batch\n",
    "errD_fake =  loss\n",
    "\n",
    "print(\"Error or loss is \",errD_fake)\n",
    "running_corrects = torch.sum(preds == label.data)\n",
    "acc=running_corrects/labels.shape[0]\n",
    "print(\"AlexNet Accuracy on prune rate\",prune_rate,\"compression\",compression,\" is \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 112.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        1888.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(preds.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
