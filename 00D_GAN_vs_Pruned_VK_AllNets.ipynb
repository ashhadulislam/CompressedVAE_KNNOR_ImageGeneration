{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models import *\n",
    "from experiment import VAEXperiment\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lib_prune\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# # Plot some training images\n",
    "# real_batch = next(iter(train_loader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create the generator\n",
    "# netG = Generator(ngpu).to(device)\n",
    "\n",
    "# # Handle multi-gpu if desired\n",
    "# if (device.type == 'cuda') and (ngpu > 1):\n",
    "#     netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# # Apply the weights_init function to randomly initialize all weights\n",
    "# #  to mean=0, stdev=0.02.\n",
    "# netG.apply(weights_init)\n",
    "\n",
    "# # Print the model\n",
    "# print(netG)\n",
    "\n",
    "\n",
    "# # In[ ]:\n",
    "\n",
    "\n",
    "# # Create the Discriminator\n",
    "# netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# # Handle multi-gpu if desired\n",
    "# if (device.type == 'cuda') and (ngpu > 1):\n",
    "#     netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# # Apply the weights_init function to randomly initialize all weights\n",
    "# #  to mean=0, stdev=0.2.\n",
    "# netD.apply(weights_init)\n",
    "\n",
    "# # Print the model\n",
    "# print(netD)\n",
    "\n",
    "\n",
    "# # In[ ]:\n",
    "\n",
    "\n",
    "# # Initialize BCELoss function\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # Create batch of latent vectors that we will use to visualize\n",
    "# #  the progression of the generator\n",
    "# fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# # Establish convention for real and fake labels during training\n",
    "# real_label = 1.\n",
    "# fake_label = 0.\n",
    "\n",
    "# gen_path=\"models/GANs/netG1.p\"\n",
    "# netG.load_state_dict(torch.load(gen_path,map_location=device))\n",
    "\n",
    "# disc_path=\"models/GANs/netD1.p\"\n",
    "# netD.load_state_dict(torch.load(disc_path,map_location=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate GAN images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_size=2000\n",
    "# noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "# # Generate fake image batch with G\n",
    "# fake = netG(noise)\n",
    "\n",
    "# # normalize the values\n",
    "# GAN_fakes=(fake-torch.min(fake))/(torch.max(fake)-torch.min(fake))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images using VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dic={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate images using Vanilla VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "logs/VanillaVAE/version_2/checkpoints/last.ckpt\n",
      "logs/VanillaVAE/version_2/checkpoints/epoch=2-step=7631.ckpt\n",
      "Generating mask\n",
      "compression is  0.21500469190262683\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n",
      "loaded model\n",
      "logs/VanillaVAE/version_2/checkpoints/last.ckpt\n",
      "logs/VanillaVAE/version_2/checkpoints/epoch=2-step=7631.ckpt\n",
      "Generating mask\n",
      "compression is  0.5218419178009135\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n"
     ]
    }
   ],
   "source": [
    "model_nm=\"VanillaVAE\"\n",
    "image_dic[model_nm]={}\n",
    "args_filename=\"configs/vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "    \n",
    "###################################\n",
    "##### Prune here\n",
    "\n",
    "prune_rates=[0.2,0.6]\n",
    "for prune_rate in prune_rates:\n",
    "    \n",
    "\n",
    "    model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_2/checkpoints/last.ckpt\"\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    for nm,params in model.named_parameters():\n",
    "        keyy=\"model.\"+nm \n",
    "        params.data=checkpoint[\"state_dict\"][keyy]\n",
    "\n",
    "    print(\"loaded model\")\n",
    "    state_dicts=[]\n",
    "    epoch_names=[\"last.ckpt\",\"epoch=2-step=7631.ckpt\"]\n",
    "\n",
    "    for epoch_name in epoch_names:\n",
    "        chk_path=\"logs/\"+model_nm+\"/version_2/checkpoints/\"+epoch_name\n",
    "        print(chk_path)\n",
    "        checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "        state_dict=checkpoint[\"state_dict\"]\n",
    "        state_dicts.append(state_dict)\n",
    "\n",
    "\n",
    "    importance_vector=[0.8,0.2]\n",
    "    evol_wts={}\n",
    "    for nm,params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            keyy=\"model.\"+nm         \n",
    "            new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "            evol_wts[nm]=new_param_values\n",
    "\n",
    "    print(\"Generating mask\")\n",
    "    list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "    model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "    total_size,nz_size=lib_prune.model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"compression is \",compression)        \n",
    "    \n",
    "    # Decode using VAE    \n",
    "\n",
    "\n",
    "    X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/pruned_\"+str(prune_rate)+\"_test_aug_enc.npy\")\n",
    "    mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mu=X_vals_enc_arr[:,:mid]\n",
    "        log_var=X_vals_enc_arr[:,mid:]\n",
    "\n",
    "        mu=torch.tensor(mu)\n",
    "        log_var=torch.tensor(log_var)\n",
    "        print(mu.shape,log_var.shape)\n",
    "        z = model.reparameterize(mu, log_var)    \n",
    "        images=model.decode(z)    \n",
    "\n",
    "    # normalize the values\n",
    "    Vanilla_images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "    image_dic[model_nm][prune_rate]=Vanilla_images\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last.ckpt', 'epoch=1-step=5087.ckpt', 'epoch=0-step=2543.ckpt']\n"
     ]
    }
   ],
   "source": [
    "chk_path\n",
    "print(os.listdir(\"logs/ConditionalVAE/version_0/checkpoints\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate images using Conditional VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "logs/ConditionalVAE/version_0/checkpoints/last.ckpt\n",
      "logs/ConditionalVAE/version_0/checkpoints/epoch=1-step=5087.ckpt\n",
      "Generating mask\n",
      "compression is  0.21541094099490637\n",
      "torch.Size([2000, 168])\n",
      "loaded model\n",
      "logs/ConditionalVAE/version_0/checkpoints/last.ckpt\n",
      "logs/ConditionalVAE/version_0/checkpoints/epoch=1-step=5087.ckpt\n",
      "Generating mask\n",
      "compression is  0.5127934990069943\n",
      "torch.Size([2000, 168])\n"
     ]
    }
   ],
   "source": [
    "model_nm=\"ConditionalVAE\"\n",
    "image_dic[model_nm]={}\n",
    "args_filename=\"configs/cvae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "###################################\n",
    "##### Prune here\n",
    "\n",
    "prune_rates=[0.2,0.6]\n",
    "prune_augmented_encoded_files=[\"pruned_0.2_0.21541094099490637test_aug_enc.npy\",\n",
    "                               \"pruned_0.6_0.5127934990069943test_aug_enc.npy\"]\n",
    "for i in range(len(prune_rates)):\n",
    "    prune_rate=prune_rates[i]\n",
    "    prune_augmented_encoded_file=prune_augmented_encoded_files[i]\n",
    "    model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    for nm,params in model.named_parameters():\n",
    "        keyy=\"model.\"+nm \n",
    "        params.data=checkpoint[\"state_dict\"][keyy]\n",
    "\n",
    "    print(\"loaded model\")\n",
    "    state_dicts=[]\n",
    "    epoch_names=[\"last.ckpt\",\"epoch=1-step=5087.ckpt\"]\n",
    "\n",
    "    for epoch_name in epoch_names:\n",
    "        chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/\"+epoch_name\n",
    "        print(chk_path)\n",
    "        checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "        state_dict=checkpoint[\"state_dict\"]\n",
    "        state_dicts.append(state_dict)\n",
    "\n",
    "\n",
    "    importance_vector=[0.8,0.2]\n",
    "    evol_wts={}\n",
    "    for nm,params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            keyy=\"model.\"+nm         \n",
    "            new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "            evol_wts[nm]=new_param_values\n",
    "\n",
    "    print(\"Generating mask\")\n",
    "    list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "    model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "    total_size,nz_size=lib_prune.model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"compression is \",compression)        \n",
    "    \n",
    "    # Decode using VAE    \n",
    "\n",
    "\n",
    "    X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/\"+prune_augmented_encoded_file)\n",
    "    mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu=X_vals_enc_arr[:,:128]\n",
    "        log_var=X_vals_enc_arr[:,128:256]\n",
    "        then_some=X_vals_enc_arr[:,256:]\n",
    "        mu=torch.tensor(mu)\n",
    "        log_var=torch.tensor(log_var)\n",
    "        then_some=torch.tensor(then_some)\n",
    "        z = model.reparameterize(mu, log_var)\n",
    "        z = torch.cat([z, then_some], dim = 1)\n",
    "        print(z.shape)\n",
    "        images=model.decode(z)        \n",
    "\n",
    "    # normalize the values\n",
    "    Vanilla_images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "    image_dic[model_nm][prune_rate]=Vanilla_images\n",
    "\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate images using DFC VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "logs/DFCVAE/version_0/checkpoints/last.ckpt\n",
      "logs/DFCVAE/version_0/checkpoints/epoch=1-step=5087.ckpt\n",
      "Generating mask\n",
      "compression is  0.21360746119711252\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n",
      "torch.Size([2000, 3, 64, 64])\n",
      "loaded model\n",
      "logs/DFCVAE/version_0/checkpoints/last.ckpt\n",
      "logs/DFCVAE/version_0/checkpoints/epoch=1-step=5087.ckpt\n",
      "Generating mask\n",
      "compression is  0.4915161398993901\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n",
      "torch.Size([2000, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "model_nm=\"DFCVAE\"\n",
    "image_dic[model_nm]={}\n",
    "args_filename=\"configs/dfc_vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "        \n",
    "        \n",
    "###################################\n",
    "##### Prune here\n",
    "\n",
    "prune_rates=[0.2,0.6]\n",
    "prune_augmented_encoded_files=[\"pruned_0.2_0.21360746119711252test_aug_enc.npy\",\n",
    "                               \"pruned_0.6_0.4915161398993901test_aug_enc.npy\"]\n",
    "for i in range(len(prune_rates)):\n",
    "    prune_rate=prune_rates[i]\n",
    "    prune_augmented_encoded_file=prune_augmented_encoded_files[i]\n",
    "    model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    for nm,params in model.named_parameters():\n",
    "        keyy=\"model.\"+nm \n",
    "        params.data=checkpoint[\"state_dict\"][keyy]\n",
    "\n",
    "    print(\"loaded model\")\n",
    "    state_dicts=[]\n",
    "    epoch_names=[\"last.ckpt\",\"epoch=1-step=5087.ckpt\"]\n",
    "\n",
    "    for epoch_name in epoch_names:\n",
    "        chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/\"+epoch_name\n",
    "        print(chk_path)\n",
    "        checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "        state_dict=checkpoint[\"state_dict\"]\n",
    "        state_dicts.append(state_dict)\n",
    "\n",
    "\n",
    "    importance_vector=[0.8,0.2]\n",
    "    evol_wts={}\n",
    "    for nm,params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            keyy=\"model.\"+nm         \n",
    "            new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "            evol_wts[nm]=new_param_values\n",
    "\n",
    "    print(\"Generating mask\")\n",
    "    list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "    model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "    total_size,nz_size=lib_prune.model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"compression is \",compression)        \n",
    "    \n",
    "    # Decode using VAE    \n",
    "\n",
    "\n",
    "    X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/\"+prune_augmented_encoded_file)\n",
    "    mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mu=X_vals_enc_arr[:,:mid]\n",
    "        log_var=X_vals_enc_arr[:,mid:]\n",
    "        mu=torch.tensor(mu)\n",
    "        log_var=torch.tensor(log_var)\n",
    "        print(mu.shape,log_var.shape)\n",
    "\n",
    "        z = model.reparameterize(mu, log_var)    \n",
    "        images=model.decode(z)  \n",
    "        print(images.shape)\n",
    "\n",
    "    # normalize the values\n",
    "    Vanilla_images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "    image_dic[model_nm][prune_rate]=Vanilla_images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate images using Beta VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "logs/BetaVAE/version_0/checkpoints/last.ckpt\n",
      "logs/BetaVAE/version_0/checkpoints/epoch=9-step=25439.ckpt\n",
      "Generating mask\n",
      "compression is  0.2344605327817332\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n",
      "loaded model\n",
      "logs/BetaVAE/version_0/checkpoints/last.ckpt\n",
      "logs/BetaVAE/version_0/checkpoints/epoch=9-step=25439.ckpt\n",
      "Generating mask\n",
      "compression is  0.5642884624908099\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n"
     ]
    }
   ],
   "source": [
    "model_nm=\"BetaVAE\"\n",
    "image_dic[model_nm]={}\n",
    "args_filename=\"configs/bbvae.yaml\"\n",
    "\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "        \n",
    "###################################\n",
    "##### Prune here\n",
    "\n",
    "prune_rates=[0.2,0.6]\n",
    "prune_augmented_encoded_files=[\"pruned_0.2_0.2344605327817332test_aug_enc.npy\",\n",
    "                               \"pruned_0.6_0.5642884624908099test_aug_enc.npy\"]\n",
    "for i in range(len(prune_rates)):\n",
    "    prune_rate=prune_rates[i]\n",
    "    prune_augmented_encoded_file=prune_augmented_encoded_files[i]\n",
    "    model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    for nm,params in model.named_parameters():\n",
    "        keyy=\"model.\"+nm \n",
    "        params.data=checkpoint[\"state_dict\"][keyy]\n",
    "\n",
    "    print(\"loaded model\")\n",
    "    state_dicts=[]\n",
    "    epoch_names=[\"last.ckpt\",\"epoch=9-step=25439.ckpt\"]\n",
    "\n",
    "    for epoch_name in epoch_names:\n",
    "        chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/\"+epoch_name\n",
    "        print(chk_path)\n",
    "        checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "        state_dict=checkpoint[\"state_dict\"]\n",
    "        state_dicts.append(state_dict)\n",
    "\n",
    "\n",
    "    importance_vector=[0.8,0.2]\n",
    "    evol_wts={}\n",
    "    for nm,params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            keyy=\"model.\"+nm         \n",
    "            new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "            evol_wts[nm]=new_param_values\n",
    "\n",
    "    print(\"Generating mask\")\n",
    "    list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "    model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "    total_size,nz_size=lib_prune.model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"compression is \",compression)        \n",
    "    \n",
    "    # Decode using VAE    \n",
    "\n",
    "\n",
    "    X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/\"+prune_augmented_encoded_file)\n",
    "    mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mu=X_vals_enc_arr[:,:mid]\n",
    "        log_var=X_vals_enc_arr[:,mid:]\n",
    "\n",
    "        mu=torch.tensor(mu)\n",
    "        log_var=torch.tensor(log_var)\n",
    "        print(mu.shape,log_var.shape)\n",
    "        z = model.reparameterize(mu, log_var)    \n",
    "        images=model.decode(z)    \n",
    "\n",
    "    # normalize the values\n",
    "    Vanilla_images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "    image_dic[model_nm][prune_rate]=Vanilla_images\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate using MIWAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causes memory problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nm=\"MIWAE\"\n",
    "# args_filename=\"configs/miwae.yaml\"\n",
    "# with open(args_filename, 'r') as file:\n",
    "#     try:\n",
    "#         config = yaml.safe_load(file)\n",
    "#     except yaml.YAMLError as exc:\n",
    "#         print(exc)\n",
    "        \n",
    "# model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "# chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "\n",
    "# checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "\n",
    "\n",
    "# for nm,params in model.named_parameters():    \n",
    "#     keyy=\"model.\"+nm \n",
    "#     params.data=checkpoint[\"state_dict\"][keyy]\n",
    "    \n",
    "    \n",
    "# X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/test_aug_enc.npy\")\n",
    "# mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     print(\"Creating images\")\n",
    "    \n",
    "#     mu=X_vals_enc_arr[:,:mid]\n",
    "#     log_var=X_vals_enc_arr[:,mid:]\n",
    "#     mu=torch.tensor(mu)\n",
    "#     log_var=torch.tensor(log_var)\n",
    "    \n",
    "#     mu = mu.repeat(model.num_estimates, model.num_samples, 1, 1).permute(2, 0, 1, 3) # [B x M x S x D]\n",
    "#     log_var = log_var.repeat(model.num_estimates, model.num_samples, 1, 1).permute(2, 0, 1, 3) # [B x M x S x D]\n",
    "#     print(mu.shape,log_var.shape)\n",
    "    \n",
    "    \n",
    "#     print(mu.shape,log_var.shape)\n",
    "#     z = model.reparameterize(mu, log_var)    \n",
    "#     print(\"Done reparam\",z.shape)\n",
    "#     images=model.decode(z)  \n",
    "#     print(\"done decoding\")\n",
    "#     print(images.shape)\n",
    "    \n",
    "#     images=images[:, 0, 0, :]\n",
    "#     print(images.shape)    \n",
    "    \n",
    "    \n",
    "# # normalize the values\n",
    "# MIWAE_images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "# image_dic[\"MIWAE_images\"]=MIWAE_images                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate using MSSIMVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "logs/MSSIMVAE/version_0/checkpoints/last.ckpt\n",
      "logs/MSSIMVAE/version_0/checkpoints/epoch=1-step=5087.ckpt\n",
      "Generating mask\n",
      "compression is  0.19029315820282988\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n",
      "torch.Size([2000, 3, 64, 64])\n",
      "loaded model\n",
      "logs/MSSIMVAE/version_0/checkpoints/last.ckpt\n",
      "logs/MSSIMVAE/version_0/checkpoints/epoch=1-step=5087.ckpt\n",
      "Generating mask\n",
      "compression is  0.4840443565744413\n",
      "torch.Size([2000, 128]) torch.Size([2000, 128])\n",
      "torch.Size([2000, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "model_nm=\"MSSIMVAE\"\n",
    "image_dic[model_nm]={}\n",
    "args_filename=\"configs/mssim_vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "        \n",
    "        \n",
    "###################################\n",
    "##### Prune here\n",
    "\n",
    "prune_rates=[0.2,0.6]\n",
    "prune_augmented_encoded_files=[\"pruned_0.2_0.19029315820282988test_aug_enc.npy\",\n",
    "                               \"pruned_0.6_0.4840443565744413test_aug_enc.npy\"]\n",
    "for i in range(len(prune_rates)):\n",
    "    prune_rate=prune_rates[i]\n",
    "    prune_augmented_encoded_file=prune_augmented_encoded_files[i]\n",
    "    model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    for nm,params in model.named_parameters():\n",
    "        keyy=\"model.\"+nm \n",
    "        params.data=checkpoint[\"state_dict\"][keyy]\n",
    "\n",
    "    print(\"loaded model\")\n",
    "    state_dicts=[]\n",
    "    epoch_names=[\"last.ckpt\",\"epoch=1-step=5087.ckpt\"]\n",
    "\n",
    "    for epoch_name in epoch_names:\n",
    "        chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/\"+epoch_name\n",
    "        print(chk_path)\n",
    "        checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "        state_dict=checkpoint[\"state_dict\"]\n",
    "        state_dicts.append(state_dict)\n",
    "\n",
    "\n",
    "    importance_vector=[0.8,0.2]\n",
    "    evol_wts={}\n",
    "    for nm,params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            keyy=\"model.\"+nm         \n",
    "            new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "            evol_wts[nm]=new_param_values\n",
    "\n",
    "    print(\"Generating mask\")\n",
    "    list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "    model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "    total_size,nz_size=lib_prune.model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"compression is \",compression)        \n",
    "    \n",
    "    # Decode using VAE    \n",
    "\n",
    "\n",
    "    X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/\"+prune_augmented_encoded_file)\n",
    "    mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mu=X_vals_enc_arr[:,:mid]\n",
    "        log_var=X_vals_enc_arr[:,mid:]\n",
    "        mu=torch.tensor(mu)\n",
    "        log_var=torch.tensor(log_var)\n",
    "\n",
    "        print(mu.shape,log_var.shape)\n",
    "        z = model.reparameterize(mu, log_var)    \n",
    "        images=model.decode(z)  \n",
    "        print(images.shape)\n",
    "\n",
    "    # normalize the values\n",
    "    Vanilla_images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "    image_dic[model_nm][prune_rate]=Vanilla_images\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate using WAE_MMD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "logs/WAE_MMD/version_0/checkpoints/last.ckpt\n",
      "logs/WAE_MMD/version_0/checkpoints/epoch=9-step=25439.ckpt\n",
      "Generating mask\n",
      "compression is  0.18048611796984407\n",
      "loaded model\n",
      "logs/WAE_MMD/version_0/checkpoints/last.ckpt\n",
      "logs/WAE_MMD/version_0/checkpoints/epoch=9-step=25439.ckpt\n",
      "Generating mask\n",
      "compression is  0.48966863953302026\n"
     ]
    }
   ],
   "source": [
    "model_nm=\"WAE_MMD\"\n",
    "image_dic[model_nm]={}\n",
    "args_filename=\"configs/wae_mmd_imq.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "###################################\n",
    "##### Prune here\n",
    "\n",
    "prune_rates=[0.2,0.6]\n",
    "prune_augmented_encoded_files=[\"pruned_0.2_0.18048611796984407test_aug_enc.npy\",\n",
    "                               \"pruned_0.6_0.48966863953302026test_aug_enc.npy\"]\n",
    "for i in range(len(prune_rates)):\n",
    "    prune_rate=prune_rates[i]\n",
    "    prune_augmented_encoded_file=prune_augmented_encoded_files[i]\n",
    "    model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    for nm,params in model.named_parameters():\n",
    "        keyy=\"model.\"+nm \n",
    "        params.data=checkpoint[\"state_dict\"][keyy]\n",
    "\n",
    "    print(\"loaded model\")\n",
    "    state_dicts=[]\n",
    "    epoch_names=[\"last.ckpt\",\"epoch=9-step=25439.ckpt\"]\n",
    "\n",
    "    for epoch_name in epoch_names:\n",
    "        chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/\"+epoch_name\n",
    "        print(chk_path)\n",
    "        checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "        state_dict=checkpoint[\"state_dict\"]\n",
    "        state_dicts.append(state_dict)\n",
    "\n",
    "\n",
    "    importance_vector=[0.8,0.2]\n",
    "    evol_wts={}\n",
    "    for nm,params in model.named_parameters():\n",
    "        if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "            keyy=\"model.\"+nm         \n",
    "            new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "            evol_wts[nm]=new_param_values\n",
    "\n",
    "    print(\"Generating mask\")\n",
    "    list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "    model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "    total_size,nz_size=lib_prune.model_size(model)\n",
    "    compression=(total_size-nz_size)/total_size\n",
    "    print(\"compression is \",compression)        \n",
    "    \n",
    "    # Decode using VAE    \n",
    "    X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/\"+prune_augmented_encoded_file)\n",
    "    mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images=model.decode(torch.tensor(X_vals_enc_arr).float())    \n",
    "    # normalize the values\n",
    "    Vanilla_images=(images-torch.min(images))/(torch.max(images)-torch.min(images))\n",
    "    image_dic[model_nm][prune_rate]=Vanilla_images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dic={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_model = models.alexnet(pretrained=True)\n",
    "# Here the size of each output sample is set to 2.\n",
    "alex_model.classifier[6] = nn.Linear(4096,2)\n",
    "alex_model = alex_model.to(device)\n",
    "PATH=\"../../AFaceDetector/models/s1.pt\"\n",
    "alex_model.load_state_dict(torch.load(PATH))\n",
    "alex_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "model_dic[\"alexnet\"]=alex_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg=models.vgg16(pretrained=True)\n",
    "model_vgg.classifier[6]=nn.Linear(4096,2)\n",
    "model_vgg=model_vgg.to(device)\n",
    "PATH=\"../../AFaceDetector/models/1_VGGnet.pt\"\n",
    "model_vgg.load_state_dict(torch.load(PATH,map_location=device))\n",
    "model_vgg.eval()\n",
    "\n",
    "\n",
    "\n",
    "model_dic[\"vggnet\"]=model_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_Resnet = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model_Resnet.fc.in_features\n",
    "# model_Resnet.fc = nn.Linear(num_ftrs, 2)\n",
    "# model_Resnet = model_Resnet.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# PATH=\"../../AFaceDetector/models/1_Renset.pt\"\n",
    "# model_Resnet.load_state_dict(torch.load(PATH,map_location=device))\n",
    "# model_Resnet.eval()\n",
    "\n",
    "\n",
    "\n",
    "# model_dic[\"resnet\"]=model_Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_denseNet = models.densenet121(pretrained=True)\n",
    "\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# model_denseNet.classifier = nn.Linear(1024, 2)\n",
    "# model_denseNet = model_denseNet.to(device)\n",
    "\n",
    "# PATH=\"../../AFaceDetector/models/1_Denseset.pt\"\n",
    "# model_denseNet.load_state_dict(torch.load(PATH,map_location=device))\n",
    "# model_denseNet.eval()\n",
    "\n",
    "\n",
    "# # densenet takes too long and kernel death\n",
    "# model_dic[\"densenet\"]=model_denseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alexnet', 'vggnet'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result on GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "# # Classify all fake batch with D\n",
    "\n",
    "# for model_name,model in model_dic.items():\n",
    "#     print(model_name)\n",
    "#     outputs = model(GAN_fakes)\n",
    "#     _,preds=torch.max(outputs,1)\n",
    "#     running_corrects = torch.sum(preds == labels.data)\n",
    "#     acc=running_corrects/labels.shape[0]\n",
    "#     print(\"DCGAN\",model_name , \"Accuracy is \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result on VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['VanillaVAE', 'ConditionalVAE', 'DFCVAE', 'BetaVAE', 'MSSIMVAE', 'WAE_MMD'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaVAE\n",
      "0.2\n",
      "VanillaVAE 0.2 alexnet Accuracy is  tensor(0.0420)\n",
      "VanillaVAE 0.2 vggnet Accuracy is  tensor(0.0070)\n",
      "0.6\n",
      "VanillaVAE 0.6 alexnet Accuracy is  tensor(0.0535)\n",
      "VanillaVAE 0.6 vggnet Accuracy is  tensor(0.0150)\n",
      "ConditionalVAE\n",
      "0.2\n",
      "ConditionalVAE 0.2 alexnet Accuracy is  tensor(0.0805)\n",
      "ConditionalVAE 0.2 vggnet Accuracy is  tensor(0.0050)\n",
      "0.6\n",
      "ConditionalVAE 0.6 alexnet Accuracy is  tensor(0.0540)\n",
      "ConditionalVAE 0.6 vggnet Accuracy is  tensor(0.0100)\n",
      "DFCVAE\n",
      "0.2\n",
      "DFCVAE 0.2 alexnet Accuracy is  tensor(0.0555)\n",
      "DFCVAE 0.2 vggnet Accuracy is  tensor(0.0605)\n",
      "0.6\n",
      "DFCVAE 0.6 alexnet Accuracy is  tensor(0.0325)\n",
      "DFCVAE 0.6 vggnet Accuracy is  tensor(0.0920)\n",
      "BetaVAE\n",
      "0.2\n",
      "BetaVAE 0.2 alexnet Accuracy is  tensor(0.0480)\n",
      "BetaVAE 0.2 vggnet Accuracy is  tensor(0.0065)\n",
      "0.6\n",
      "BetaVAE 0.6 alexnet Accuracy is  tensor(0.0365)\n",
      "BetaVAE 0.6 vggnet Accuracy is  tensor(0.0480)\n",
      "MSSIMVAE\n",
      "0.2\n",
      "MSSIMVAE 0.2 alexnet Accuracy is  tensor(0.0800)\n",
      "MSSIMVAE 0.2 vggnet Accuracy is  tensor(0.0045)\n",
      "0.6\n",
      "MSSIMVAE 0.6 alexnet Accuracy is  tensor(0.0400)\n",
      "MSSIMVAE 0.6 vggnet Accuracy is  tensor(0.0245)\n",
      "WAE_MMD\n",
      "0.2\n",
      "WAE_MMD 0.2 alexnet Accuracy is  tensor(0.0335)\n",
      "WAE_MMD 0.2 vggnet Accuracy is  tensor(0.0045)\n",
      "0.6\n",
      "WAE_MMD 0.6 alexnet Accuracy is  tensor(0.0210)\n",
      "WAE_MMD 0.6 vggnet Accuracy is  tensor(0.1205)\n"
     ]
    }
   ],
   "source": [
    "fake_label=0\n",
    "for vae_model_name,rest in image_dic.items():\n",
    "    print(vae_model_name)\n",
    "    for prune_rate,images_vae in rest.items():\n",
    "        print(prune_rate)\n",
    "        b_size=images_vae.shape[0]\n",
    "        labels = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "        for model_name,model in model_dic.items():\n",
    "            outputs = model(images_vae)\n",
    "            _,preds=torch.max(outputs,1)\n",
    "            running_corrects = torch.sum(preds == labels.data)\n",
    "            acc=running_corrects/labels.shape[0]\n",
    "            print(vae_model_name,prune_rate,model_name,\"Accuracy is \",acc)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name,images_vae in image_dic.items():\n",
    "# #     print(name,images_vae.shape)\n",
    "#     b_size=images_vae.shape[0]\n",
    "#     labels = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "    \n",
    "#     for model_name,model in model_dic.items():\n",
    "#         outputs = model(images_vae)\n",
    "#         _,preds=torch.max(outputs,1)\n",
    "#         running_corrects = torch.sum(preds == labels.data)\n",
    "#         acc=running_corrects/labels.shape[0]\n",
    "#         print(name,model_name,\"Accuracy is \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "# # Classify all fake batch with D\n",
    "# outputs = alex_model(Vanilla_images)\n",
    "# _,preds=torch.max(outputs,1)\n",
    "# running_corrects = torch.sum(preds == labels.data)\n",
    "# acc=running_corrects/labels.shape[0]\n",
    "# print(\"Accuracy is \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# b_size=ConditionalVAE_images.shape[0]\n",
    "# labels = torch.full((b_size,), fake_label, dtype=torch.long, device=device)\n",
    "# # Classify all fake batch with D\n",
    "# outputs = alex_model(ConditionalVAE_images)\n",
    "# _,preds=torch.max(outputs,1)\n",
    "# running_corrects = torch.sum(preds == labels.data)\n",
    "# acc=running_corrects/labels.shape[0]\n",
    "# print(\"Accuracy is \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
