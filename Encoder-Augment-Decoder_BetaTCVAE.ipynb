{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models import *\n",
    "from experiment import VAEXperiment\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict_classification(X,y,new_vector, num_neighbors_to_test,expected_class_index):\n",
    "    '''\n",
    "    this function is used to validate\n",
    "    whether new point generated is close to\n",
    "    same label points\n",
    "    '''\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    posit=np.argsort(abs((X-new_vector)*(X-new_vector)).sum(axis=1))\n",
    "    classes = y[posit[0:num_neighbors_to_test]]\n",
    "    return np.sum(classes==expected_class_index)==classes.shape[0]\n",
    "\n",
    "def check_duplicates( new_row,old_rows):\n",
    "    '''\n",
    "    check if the new row\n",
    "    is already preent in the old rows\n",
    "    '''\n",
    "    for row in old_rows:\n",
    "        same=True\n",
    "        for i in range(len(row)):\n",
    "            if new_row[i]!=row[i]:\n",
    "                same=False\n",
    "                continue\n",
    "        if same:\n",
    "            return True                            \n",
    "    return False\n",
    "\n",
    "def get_minority_label_index(X,y):\n",
    "    '''\n",
    "    find the minority label\n",
    "    and the indices at which minority label\n",
    "    is present\n",
    "    '''\n",
    "    # find the minority label\n",
    "    uniq_labels=np.unique(y)\n",
    "    # count for each label\n",
    "    dic_nry={}\n",
    "\n",
    "    for uniq_label in uniq_labels:\n",
    "        dic_nry[uniq_label]=0\n",
    "\n",
    "    for y_val in y:\n",
    "        dic_nry[y_val]+=1\n",
    "\n",
    "    # then which one is the minority label?\n",
    "    minority_label=-1\n",
    "    minimum_count=np.inf\n",
    "    for k,v in dic_nry.items():\n",
    "        if minimum_count>v:\n",
    "            minimum_count=v\n",
    "            minority_label=k\n",
    "\n",
    "\n",
    "    # now get the indices of the minority labels\n",
    "    minority_indices=[]\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i]==minority_label:\n",
    "            minority_indices.append(i)\n",
    "\n",
    "    return minority_label,minority_indices\n",
    "\n",
    "def good_count_neighbors(X,y):\n",
    "    '''\n",
    "    find the good number of neighbors to use\n",
    "    this function is used on auto pilot\n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    count_greater=y_minority.shape[0]\n",
    "    for i in range(X_minority.shape[0]):\n",
    "        this_point_features=X_minority[i]\n",
    "        dist = ((X_minority-this_point_features)*(X_minority-this_point_features)).sum(axis=1)\n",
    "        mean_dist=np.mean(dist)\n",
    "#         print(dist,mean_dist)\n",
    "        this_point_count_lesser = (dist < mean_dist).sum()\n",
    "        count_greater=min(this_point_count_lesser,count_greater)        \n",
    "    return count_greater\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# following function\n",
    "# to get the savitzky golay filter\n",
    "# https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter\n",
    "# https://scipy.github.io/old-wiki/pages/Cookbook/SavitzkyGolay\n",
    "# https://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-in-the-right-way\n",
    "\n",
    "def savitzky_golay(y, window_size, order, deriv=0, rate=1):         \n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "\n",
    "    try:\n",
    "        window_size = np.abs(int(window_size))\n",
    "        order = np.abs(int(order))\n",
    "    except ValueError:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with\n",
    "    # values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')\n",
    "\n",
    "\n",
    "def check_enough_minorities(X,y,num_neighbors):\n",
    "    '''\n",
    "    ideally, the total number of minority points should be\n",
    "    1 more than the total number of neighbors    \n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    if len(minority_indices)<=num_neighbors:\n",
    "        print(\"You want to use \",num_neighbors,\"neighbors, but minority data size = \",len(minority_indices))\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def calculate_count_to_add(X,y,final_proportion):\n",
    "    '''\n",
    "    Calculate the number of artificial points to be generated so that\n",
    "    (count_minority_existing+count_artificial_minority)/count_majority_existing=final_proportion\n",
    "    '''\n",
    "#     minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "#     majority_indices=[]\n",
    "#     for i in range(0,y.shape[0]):\n",
    "#         if i not in minority_indices:\n",
    "#             majority_indices.append(i)\n",
    "#     count_minority=len(minority_indices)\n",
    "#     count_majority=len(majority_indices)\n",
    "#     new_minority=int((final_proportion*count_majority)-count_minority)\n",
    "#     if new_minority<1:\n",
    "#         return -1\n",
    "    \n",
    "    \n",
    "    # extra code\n",
    "    count_to_add=int(final_proportion*len(X))\n",
    "    return count_to_add\n",
    "    \n",
    "#     return new_minority\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_distance_threshold(X,y,num_neighbors,intra=True):\n",
    "    '''\n",
    "    returns the distance threshold, based on the intra parameter\n",
    "    if intra is chosen, returns the cut-off point for distances to\n",
    "    kth nearest neighbor of same class\n",
    "    in inter is chosen, returns the cut-off point for distances to \n",
    "    kth nearest neighbor of opposite class\n",
    "\n",
    "    '''\n",
    "    win_size=5 #positive odd number\n",
    "    pol_order=2\n",
    "    alpha=0.0001 # low value for denominator 0 case\n",
    "    minortiy_label=1\n",
    "    minority_indices=list(range(0,len(X)))\n",
    "#     minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    \n",
    "\n",
    "    if intra:\n",
    "        internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "        internal_distance = np.sort(internal_distance)\n",
    "        knd=internal_distance[:,num_neighbors]\n",
    "\n",
    "        knd_sorted = np.sort(knd)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # normalize it        \n",
    "    normalized_dist= (knd_sorted-np.min(knd_sorted))/(np.max(knd_sorted)-np.min(knd_sorted)+alpha)\n",
    "\n",
    "    # apply golay        \n",
    "    normalized_dist = savitzky_golay(normalized_dist, win_size, pol_order) # window size 51, polynomial order 3\n",
    "    plt.plot(normalized_dist)\n",
    "    plt.title(\"NOrmalized distance intra\"+str(intra))\n",
    "    plt.show()\n",
    "    normalized_dist=np.diff(normalized_dist)\n",
    "\n",
    "    sin_values=np.abs(np.sin(np.arctan(normalized_dist)))\n",
    "    plt.title(\"Sin differential - to get maxima intra\"+str(intra))\n",
    "    plt.plot(sin_values)\n",
    "    plt.show()\n",
    "    first_maxima_index=np.argmax(sin_values)\n",
    "    print(\"Maxima is at \",first_maxima_index)\n",
    "    proportion=first_maxima_index/sin_values.shape[0]\n",
    "    return proportion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# following function to calculate maximum\n",
    "# threshold distance\n",
    "# while placing a point\n",
    "def max_threshold_dist(X,y,num_neighbors):\n",
    "    '''\n",
    "    This function calculates the maximum distance between any two points in the minority class\n",
    "    It also calculates the minimum distance between a point in the minority and a point\n",
    "    in the majority class\n",
    "    the value returned is the minimum of the two\n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    majority_indices=[]\n",
    "    for i in range(0,y.shape[0]):\n",
    "        if i not in minority_indices:\n",
    "            majority_indices.append(i)\n",
    "    print(len(majority_indices),len(minority_indices),y.shape)\n",
    "    X_majority=X[majority_indices]\n",
    "    y_majority=y[majority_indices]\n",
    "\n",
    "\n",
    "\n",
    "    # calculate inter distance\n",
    "    internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "    internal_distance=internal_distance.flatten()\n",
    "    max_internal_distance=np.max(internal_distance)\n",
    "    \n",
    "    min_internal_distance=np.min(internal_distance[internal_distance>0])    \n",
    "\n",
    "\n",
    "\n",
    "    # additional code change\n",
    "    max_allowed_distance=min_internal_distance/max_internal_distance\n",
    "    \n",
    "    return max_allowed_distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm=\"BetaTCVAE\"\n",
    "args_filename=\"configs/betatc_vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "\n",
    "\n",
    "for nm,params in model.named_parameters():\n",
    "#     print(nm)\n",
    "#     print(\"model.\"+nm in checkpoint[\"state_dict\"])\n",
    "    keyy=\"model.\"+nm \n",
    "    params.data=checkpoint[\"state_dict\"][keyy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BetaTCVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc_mu): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc_var): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (decoder_input): Linear(in_features=128, out_features=512, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test images...\n",
      "2 torch.Size([144, 3, 64, 64]) torch.Size([144, 40])\n",
      "2 torch.Size([144, 128]) torch.Size([144, 128])\n"
     ]
    }
   ],
   "source": [
    "data = VAEDataset(**config[\"data_params\"])\n",
    "data.setup()\n",
    "tloader=data.test_dataloader()\n",
    "\n",
    "X_vals=[]\n",
    "print(\"Encoding test images...\")\n",
    "with torch.no_grad():\n",
    "    for nxt in tloader:\n",
    "        print(len(nxt),nxt[0].shape,nxt[1].shape)\n",
    "        enc=model.encode(nxt[0])\n",
    "        \n",
    "        print(len(enc),enc[0].shape,enc[1].shape)\n",
    "        break\n",
    "        enc_batch=torch.cat(enc,1)\n",
    "        enc_batch=enc_batch.detach().numpy()\n",
    "        X_vals.append(enc_batch)\n",
    "        \n",
    "# print(\"Completed...\")\n",
    "# X_vals_arr=np.concatenate(X_vals)\n",
    "\n",
    "# if not os.path.isdir(\"logs/\"+model_nm+\"/enc\"):\n",
    "#     os.mkdir(\"logs/\"+model_nm+\"/enc\")\n",
    "    \n",
    "# np.save(\"logs/\"+model_nm+\"/enc/test_enc.npy\",X_vals_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       " \n",
       " \n",
       "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       " \n",
       " \n",
       "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       " \n",
       " \n",
       "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       " \n",
       " \n",
       "         [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       " \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]]]], grad_fn=<TanhBackward>),\n",
       " tensor([[[[0.3255, 0.3098, 0.3882,  ..., 0.4392, 0.4431, 0.4431],\n",
       "           [0.3176, 0.3529, 0.4941,  ..., 0.4510, 0.4510, 0.4588],\n",
       "           [0.3294, 0.4627, 0.6667,  ..., 0.4549, 0.4588, 0.4667],\n",
       "           ...,\n",
       "           [0.8549, 0.8157, 0.7882,  ..., 0.0235, 0.0431, 0.0745],\n",
       "           [0.8980, 0.8549, 0.8353,  ..., 0.0314, 0.0549, 0.0784],\n",
       "           [0.9137, 0.8902, 0.8627,  ..., 0.0392, 0.0667, 0.0824]],\n",
       " \n",
       "          [[0.4118, 0.3843, 0.4510,  ..., 0.5294, 0.5333, 0.5216],\n",
       "           [0.3922, 0.4118, 0.5333,  ..., 0.5412, 0.5412, 0.5333],\n",
       "           [0.3882, 0.4902, 0.6706,  ..., 0.5451, 0.5529, 0.5333],\n",
       "           ...,\n",
       "           [0.7020, 0.6510, 0.6118,  ..., 0.0039, 0.0078, 0.0118],\n",
       "           [0.7529, 0.7020, 0.6706,  ..., 0.0118, 0.0157, 0.0157],\n",
       "           [0.7725, 0.7412, 0.7059,  ..., 0.0196, 0.0235, 0.0196]],\n",
       " \n",
       "          [[0.4549, 0.4314, 0.4784,  ..., 0.5608, 0.5686, 0.5569],\n",
       "           [0.4235, 0.4392, 0.5451,  ..., 0.5765, 0.5843, 0.5725],\n",
       "           [0.4039, 0.5020, 0.6706,  ..., 0.5804, 0.5922, 0.5725],\n",
       "           ...,\n",
       "           [0.5843, 0.5294, 0.4667,  ..., 0.0275, 0.0275, 0.0235],\n",
       "           [0.6471, 0.5882, 0.5412,  ..., 0.0314, 0.0392, 0.0275],\n",
       "           [0.6824, 0.6431, 0.5922,  ..., 0.0314, 0.0392, 0.0314]]],\n",
       " \n",
       " \n",
       "         [[[0.0353, 0.0353, 0.0353,  ..., 0.0196, 0.0235, 0.0235],\n",
       "           [0.0353, 0.0353, 0.0353,  ..., 0.0275, 0.0196, 0.0275],\n",
       "           [0.0353, 0.0314, 0.0314,  ..., 0.0275, 0.0275, 0.0235],\n",
       "           ...,\n",
       "           [0.2235, 0.2078, 0.2039,  ..., 0.0392, 0.0392, 0.0431],\n",
       "           [0.2118, 0.2078, 0.2039,  ..., 0.0353, 0.0392, 0.0431],\n",
       "           [0.2157, 0.2000, 0.2118,  ..., 0.0392, 0.0392, 0.0431]],\n",
       " \n",
       "          [[0.0471, 0.0471, 0.0471,  ..., 0.0471, 0.0392, 0.0392],\n",
       "           [0.0471, 0.0471, 0.0471,  ..., 0.0510, 0.0431, 0.0431],\n",
       "           [0.0471, 0.0431, 0.0431,  ..., 0.0431, 0.0471, 0.0431],\n",
       "           ...,\n",
       "           [0.2157, 0.2039, 0.2039,  ..., 0.0667, 0.0667, 0.0627],\n",
       "           [0.2039, 0.2039, 0.2000,  ..., 0.0627, 0.0627, 0.0627],\n",
       "           [0.2078, 0.1961, 0.2078,  ..., 0.0667, 0.0667, 0.0627]],\n",
       " \n",
       "          [[0.3098, 0.3098, 0.3098,  ..., 0.2902, 0.2863, 0.2824],\n",
       "           [0.3098, 0.3098, 0.3098,  ..., 0.2863, 0.2824, 0.2824],\n",
       "           [0.3098, 0.3059, 0.3059,  ..., 0.2745, 0.2824, 0.2745],\n",
       "           ...,\n",
       "           [0.3451, 0.3333, 0.3294,  ..., 0.2863, 0.2863, 0.2902],\n",
       "           [0.3255, 0.3294, 0.3255,  ..., 0.2784, 0.2824, 0.2902],\n",
       "           [0.3294, 0.3176, 0.3294,  ..., 0.2824, 0.2863, 0.2902]]],\n",
       " \n",
       " \n",
       "         [[[0.1412, 0.1412, 0.1373,  ..., 0.0000, 0.0078, 0.1176],\n",
       "           [0.1451, 0.1412, 0.1373,  ..., 0.0000, 0.0078, 0.1098],\n",
       "           [0.1451, 0.1490, 0.1451,  ..., 0.0000, 0.0039, 0.1176],\n",
       "           ...,\n",
       "           [0.3412, 0.3255, 0.3216,  ..., 0.0118, 0.0078, 0.0118],\n",
       "           [0.3451, 0.3333, 0.3255,  ..., 0.0157, 0.0078, 0.0196],\n",
       "           [0.3490, 0.3373, 0.3294,  ..., 0.0196, 0.0118, 0.0196]],\n",
       " \n",
       "          [[0.3725, 0.3765, 0.3686,  ..., 0.3176, 0.3255, 0.3451],\n",
       "           [0.3725, 0.3765, 0.3686,  ..., 0.3176, 0.3255, 0.3412],\n",
       "           [0.3765, 0.3843, 0.3765,  ..., 0.3176, 0.3216, 0.3412],\n",
       "           ...,\n",
       "           [0.5137, 0.5059, 0.5059,  ..., 0.3373, 0.3412, 0.3333],\n",
       "           [0.5176, 0.5137, 0.5059,  ..., 0.3412, 0.3451, 0.3373],\n",
       "           [0.5216, 0.5176, 0.5098,  ..., 0.3451, 0.3490, 0.3373]],\n",
       " \n",
       "          [[0.5176, 0.5216, 0.5176,  ..., 0.5020, 0.5020, 0.4392],\n",
       "           [0.5216, 0.5216, 0.5176,  ..., 0.5020, 0.5020, 0.4314],\n",
       "           [0.5216, 0.5294, 0.5255,  ..., 0.5020, 0.4980, 0.4275],\n",
       "           ...,\n",
       "           [0.6471, 0.6431, 0.6392,  ..., 0.5333, 0.5176, 0.5216],\n",
       "           [0.6510, 0.6471, 0.6392,  ..., 0.5373, 0.5176, 0.5294],\n",
       "           [0.6549, 0.6510, 0.6431,  ..., 0.5412, 0.5216, 0.5333]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.3059, 0.3098, 0.3176,  ..., 0.3333, 0.3333, 0.3333],\n",
       "           [0.3059, 0.3098, 0.3098,  ..., 0.3333, 0.3333, 0.3333],\n",
       "           [0.3059, 0.3137, 0.3098,  ..., 0.3294, 0.3373, 0.3333],\n",
       "           ...,\n",
       "           [0.0706, 0.0627, 0.0667,  ..., 0.3294, 0.3255, 0.3216],\n",
       "           [0.0706, 0.0627, 0.0667,  ..., 0.3294, 0.3255, 0.3176],\n",
       "           [0.0706, 0.0627, 0.0667,  ..., 0.3255, 0.3216, 0.3137]],\n",
       " \n",
       "          [[0.5255, 0.5294, 0.5373,  ..., 0.5647, 0.5647, 0.5647],\n",
       "           [0.5255, 0.5294, 0.5294,  ..., 0.5647, 0.5647, 0.5647],\n",
       "           [0.5255, 0.5333, 0.5294,  ..., 0.5608, 0.5686, 0.5647],\n",
       "           ...,\n",
       "           [0.0784, 0.0706, 0.0706,  ..., 0.5569, 0.5569, 0.5608],\n",
       "           [0.0745, 0.0667, 0.0706,  ..., 0.5569, 0.5569, 0.5569],\n",
       "           [0.0745, 0.0667, 0.0706,  ..., 0.5529, 0.5529, 0.5529]],\n",
       " \n",
       "          [[0.8196, 0.8235, 0.8314,  ..., 0.8784, 0.8784, 0.8784],\n",
       "           [0.8196, 0.8235, 0.8235,  ..., 0.8784, 0.8784, 0.8784],\n",
       "           [0.8196, 0.8275, 0.8235,  ..., 0.8745, 0.8824, 0.8784],\n",
       "           ...,\n",
       "           [0.1020, 0.0941, 0.0941,  ..., 0.8784, 0.8784, 0.8784],\n",
       "           [0.0941, 0.0863, 0.0902,  ..., 0.8784, 0.8745, 0.8745],\n",
       "           [0.0941, 0.0863, 0.0902,  ..., 0.8745, 0.8745, 0.8706]]],\n",
       " \n",
       " \n",
       "         [[[1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 1.0000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9922, 1.0000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9804, 0.9922, 1.0000],\n",
       "           ...,\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6667, 0.6902, 0.7098],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6706, 0.6784, 0.6667],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6706, 0.6667, 0.5725]],\n",
       " \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 1.0000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.9961, 1.0000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9882, 0.9961, 1.0000],\n",
       "           ...,\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6588, 0.6627, 0.6627],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6627, 0.6667, 0.6667],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6627, 0.6784, 0.6275]],\n",
       " \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.9882, 0.9882, 1.0000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9765, 0.9843, 1.0000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9529, 0.9725, 1.0000],\n",
       "           ...,\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6784, 0.6745, 0.6627],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6824, 0.6863, 0.7020],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6824, 0.7137, 0.7020]]],\n",
       " \n",
       " \n",
       "         [[[0.3647, 0.3647, 0.3647,  ..., 0.2431, 0.2392, 0.2431],\n",
       "           [0.3608, 0.3647, 0.3647,  ..., 0.2431, 0.2392, 0.2471],\n",
       "           [0.3686, 0.3647, 0.3647,  ..., 0.2392, 0.2431, 0.2510],\n",
       "           ...,\n",
       "           [0.3765, 0.3765, 0.3608,  ..., 0.2510, 0.2784, 0.3412],\n",
       "           [0.3961, 0.3804, 0.3569,  ..., 0.2667, 0.2902, 0.2863],\n",
       "           [0.3961, 0.3843, 0.3686,  ..., 0.2706, 0.2824, 0.2745]],\n",
       " \n",
       "          [[0.2392, 0.2392, 0.2392,  ..., 0.1333, 0.1294, 0.1333],\n",
       "           [0.2353, 0.2392, 0.2392,  ..., 0.1333, 0.1294, 0.1373],\n",
       "           [0.2431, 0.2392, 0.2392,  ..., 0.1294, 0.1333, 0.1412],\n",
       "           ...,\n",
       "           [0.2510, 0.2627, 0.2510,  ..., 0.1255, 0.1412, 0.1922],\n",
       "           [0.2706, 0.2667, 0.2471,  ..., 0.1412, 0.1529, 0.1373],\n",
       "           [0.2706, 0.2667, 0.2510,  ..., 0.1451, 0.1451, 0.1294]],\n",
       " \n",
       "          [[0.1412, 0.1451, 0.1490,  ..., 0.0863, 0.0824, 0.0902],\n",
       "           [0.1373, 0.1451, 0.1490,  ..., 0.0863, 0.0824, 0.0902],\n",
       "           [0.1451, 0.1490, 0.1490,  ..., 0.0824, 0.0863, 0.0941],\n",
       "           ...,\n",
       "           [0.1686, 0.1686, 0.1529,  ..., 0.0588, 0.0627, 0.1059],\n",
       "           [0.1882, 0.1647, 0.1412,  ..., 0.0745, 0.0784, 0.0588],\n",
       "           [0.1843, 0.1647, 0.1412,  ..., 0.0784, 0.0706, 0.0510]]]]),\n",
       " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward>),\n",
       " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward>),\n",
       " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(nxt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the encoded and Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many=100\n",
    "X_vals_arr=np.load(\"logs/\"+model_nm+\"/enc/test_enc.npy\")\n",
    "print(X_vals_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vals_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X=deepcopy(X_vals_arr)\n",
    "y=np.array([0 for i in range(len(X))])\n",
    "\n",
    "\n",
    "random_indices = np.random.choice(len(X), size=how_many, replace=False)\n",
    "\n",
    "X = X[random_indices, :]\n",
    "y=np.array([0 for i in range(len(X))])\n",
    "\n",
    "print(\"X=\",X.shape,\"y=\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_proportion=2\n",
    "num_neighbors=10\n",
    "n_to_sample=calculate_count_to_add(X,y,final_proportion)\n",
    "print(\"Number of new points=\",n_to_sample)\n",
    "max_dist_point=max_threshold_dist(X,y,num_neighbors)\n",
    "print(\"max_dist_point\",max_dist_point)\n",
    "proportion_intra=calculate_distance_threshold(X,y,num_neighbors,intra=True)\n",
    "proportion_minority=proportion_intra\n",
    "print(\"Proportion of population used = \",proportion_minority)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_minority=X\n",
    "y_minority=y\n",
    "\n",
    "\n",
    "\n",
    "internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "internal_distance = np.sort(internal_distance)\n",
    "knd=internal_distance[:,num_neighbors]        \n",
    "knd_sorted = np.sort(knd)        \n",
    "\n",
    "\n",
    "threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]\n",
    "print(\"Threshold distance is \",threshold_dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_cannot_use=10\n",
    "original_n_neighbors=num_neighbors\n",
    "original_max_dist_point=max_dist_point\n",
    "original_proportion=proportion_minority\n",
    "X_new_minority=[]\n",
    "N = n_to_sample\n",
    "consecutive_cannot_use=0\n",
    "while N>0:\n",
    "    for i in range(X_minority.shape[0]):\n",
    "\n",
    "        if knd[i]>threshold_dist:\n",
    "            continue\n",
    "        if N==0:\n",
    "            break\n",
    "        v = X_minority[i,:]\n",
    "        val=np.sort( abs((X_minority-v)*(X_minority-v)).sum(axis=1) )\n",
    "        # sort neighbors by distance\n",
    "        # obviously will have to ignore the \n",
    "        # first term as its a distance to iteself\n",
    "        # which wil be 0\n",
    "        posit=np.argsort(abs((X_minority-v)*(X_minority-v)).sum(axis=1))\n",
    "        kv = X_minority[posit[1:num_neighbors+1],:]\n",
    "        alphak = random.uniform(0,max_dist_point)\n",
    "        m0 = v\n",
    "#         print(m0)\n",
    "        for j in range(num_neighbors):\n",
    "#             print(kv[j,:] ,\"-\", m0)\n",
    "#             print(m0,\"+\",alphak,\"*\", (kv[j,:] - m0))\n",
    "            m1 = m0 + alphak * (kv[j,:] - m0)\n",
    "            m0 = m1\n",
    "#             print(\"res\",m0)\n",
    "        num_neighbors_to_test=math.floor(math.sqrt(num_neighbors))\n",
    "        can_use= not(check_duplicates(m0,X_minority))\n",
    "        can_use=can_use and not(check_duplicates(m0,X_new_minority))                            \n",
    "        if can_use:\n",
    "            consecutive_cannot_use=0\n",
    "            num_neighbors=min(num_neighbors+1,original_n_neighbors)\n",
    "            max_dist_point=min(max_dist_point+0.01,original_max_dist_point)\n",
    "            proportion_minority=max(proportion_minority-0.01,original_proportion)\n",
    "            threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]                \n",
    "#             print(m0)\n",
    "#             print(\"*\"*10)\n",
    "            X_new_minority.append(m0)\n",
    "            N-=1\n",
    "        else:\n",
    "            consecutive_cannot_use+=1\n",
    "            if consecutive_cannot_use>=threshold_cannot_use:\n",
    "                num_neighbors=max(num_neighbors-1,2)\n",
    "                max_dist_point=max(max_dist_point-0.01,0.01)\n",
    "                proportion_minority=min(proportion_minority+0.01,0.9)\n",
    "                threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]\n",
    "                consecutive_cannot_use=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_minority=np.array(X_new_minority)\n",
    "np.save(\"logs/\"+model_nm+\"/enc/test_aug_enc.npy\",X_new_minority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode using The VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/test_aug_enc.npy\")\n",
    "mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:mid]\n",
    "    log_var=X_vals_enc_arr[:,mid:]\n",
    "\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    print(mu.shape,log_var.shape)\n",
    "    z = model.reparameterize(mu, log_var)    \n",
    "    images=model.decode(z)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows=5\n",
    "num_cols=5\n",
    "how_many=num_rows*num_cols\n",
    "\n",
    "random_indices = np.random.choice(len(images), size=how_many, replace=False)\n",
    "\n",
    "images_selected = images[random_indices, :]\n",
    "fig, axs = plt.subplots(num_rows,num_cols)\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axs[i][j].imshow(images[i*num_cols+j].permute(1,2,0))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "original_n_neighbors=num_neighbors\n",
    "original_max_dist_point=max_dist_point\n",
    "original_proportion=proportion_minority\n",
    "\n",
    "name=\"logs/\"+model_nm+\"/artificial/numnbrs_\"+str(original_n_neighbors)+\"_max_dist\"+str(original_max_dist_point)+\"_prop\"+str(original_proportion)\n",
    "name+=\"_sample_size\"+str(how_many)\n",
    "plt.savefig(name+\".pdf\")\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:128]\n",
    "    log_var=X_vals_enc_arr[:,128:256]\n",
    "\n",
    "    then_some=X_vals_enc_arr[:,256:]\n",
    "\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    then_some=torch.tensor(then_some)\n",
    "    z = model.reparameterize(mu, log_var)\n",
    "    z = torch.cat([z, then_some], dim = 1)\n",
    "    print(z.shape)\n",
    "    images=model.decode(z)    \n",
    "    print(images.shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/artificial/all_imgs/\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/artificial/all_imgs/\")\n",
    "\n",
    "\n",
    "    \n",
    "for img in images:\n",
    "    loc=\"logs/\"+model_nm+\"/artificial/all_imgs/\"    \n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.permute(1,2,0)\n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.detach().numpy()\n",
    "#     print(\"shp is \",img.shape,np.min(img),np.max(img))  \n",
    "    if np.min(img)<0 or np.max(img):\n",
    "        img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
    "    \n",
    "    plt.imsave(loc+\"img\"+str(counter)+\".jpeg\",img)\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "    counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
