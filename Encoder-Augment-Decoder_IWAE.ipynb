{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models import *\n",
    "from experiment import VAEXperiment\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict_classification(X,y,new_vector, num_neighbors_to_test,expected_class_index):\n",
    "    '''\n",
    "    this function is used to validate\n",
    "    whether new point generated is close to\n",
    "    same label points\n",
    "    '''\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    posit=np.argsort(abs((X-new_vector)*(X-new_vector)).sum(axis=1))\n",
    "    classes = y[posit[0:num_neighbors_to_test]]\n",
    "    return np.sum(classes==expected_class_index)==classes.shape[0]\n",
    "\n",
    "def check_duplicates( new_row,old_rows):\n",
    "    '''\n",
    "    check if the new row\n",
    "    is already preent in the old rows\n",
    "    '''\n",
    "    for row in old_rows:\n",
    "        same=True\n",
    "        for i in range(len(row)):\n",
    "            if new_row[i]!=row[i]:\n",
    "                same=False\n",
    "                continue\n",
    "        if same:\n",
    "            return True                            \n",
    "    return False\n",
    "\n",
    "def get_minority_label_index(X,y):\n",
    "    '''\n",
    "    find the minority label\n",
    "    and the indices at which minority label\n",
    "    is present\n",
    "    '''\n",
    "    # find the minority label\n",
    "    uniq_labels=np.unique(y)\n",
    "    # count for each label\n",
    "    dic_nry={}\n",
    "\n",
    "    for uniq_label in uniq_labels:\n",
    "        dic_nry[uniq_label]=0\n",
    "\n",
    "    for y_val in y:\n",
    "        dic_nry[y_val]+=1\n",
    "\n",
    "    # then which one is the minority label?\n",
    "    minority_label=-1\n",
    "    minimum_count=np.inf\n",
    "    for k,v in dic_nry.items():\n",
    "        if minimum_count>v:\n",
    "            minimum_count=v\n",
    "            minority_label=k\n",
    "\n",
    "\n",
    "    # now get the indices of the minority labels\n",
    "    minority_indices=[]\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i]==minority_label:\n",
    "            minority_indices.append(i)\n",
    "\n",
    "    return minority_label,minority_indices\n",
    "\n",
    "def good_count_neighbors(X,y):\n",
    "    '''\n",
    "    find the good number of neighbors to use\n",
    "    this function is used on auto pilot\n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    count_greater=y_minority.shape[0]\n",
    "    for i in range(X_minority.shape[0]):\n",
    "        this_point_features=X_minority[i]\n",
    "        dist = ((X_minority-this_point_features)*(X_minority-this_point_features)).sum(axis=1)\n",
    "        mean_dist=np.mean(dist)\n",
    "#         print(dist,mean_dist)\n",
    "        this_point_count_lesser = (dist < mean_dist).sum()\n",
    "        count_greater=min(this_point_count_lesser,count_greater)        \n",
    "    return count_greater\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# following function\n",
    "# to get the savitzky golay filter\n",
    "# https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter\n",
    "# https://scipy.github.io/old-wiki/pages/Cookbook/SavitzkyGolay\n",
    "# https://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-in-the-right-way\n",
    "\n",
    "def savitzky_golay(y, window_size, order, deriv=0, rate=1):         \n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "\n",
    "    try:\n",
    "        window_size = np.abs(int(window_size))\n",
    "        order = np.abs(int(order))\n",
    "    except ValueError:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with\n",
    "    # values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')\n",
    "\n",
    "\n",
    "def check_enough_minorities(X,y,num_neighbors):\n",
    "    '''\n",
    "    ideally, the total number of minority points should be\n",
    "    1 more than the total number of neighbors    \n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    if len(minority_indices)<=num_neighbors:\n",
    "        print(\"You want to use \",num_neighbors,\"neighbors, but minority data size = \",len(minority_indices))\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def calculate_count_to_add(X,y,final_proportion):\n",
    "    '''\n",
    "    Calculate the number of artificial points to be generated so that\n",
    "    (count_minority_existing+count_artificial_minority)/count_majority_existing=final_proportion\n",
    "    '''\n",
    "#     minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "#     majority_indices=[]\n",
    "#     for i in range(0,y.shape[0]):\n",
    "#         if i not in minority_indices:\n",
    "#             majority_indices.append(i)\n",
    "#     count_minority=len(minority_indices)\n",
    "#     count_majority=len(majority_indices)\n",
    "#     new_minority=int((final_proportion*count_majority)-count_minority)\n",
    "#     if new_minority<1:\n",
    "#         return -1\n",
    "    \n",
    "    \n",
    "    # extra code\n",
    "    count_to_add=int(final_proportion*len(X))\n",
    "    return count_to_add\n",
    "    \n",
    "#     return new_minority\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_distance_threshold(X,y,num_neighbors,intra=True):\n",
    "    '''\n",
    "    returns the distance threshold, based on the intra parameter\n",
    "    if intra is chosen, returns the cut-off point for distances to\n",
    "    kth nearest neighbor of same class\n",
    "    in inter is chosen, returns the cut-off point for distances to \n",
    "    kth nearest neighbor of opposite class\n",
    "\n",
    "    '''\n",
    "    win_size=5 #positive odd number\n",
    "    pol_order=2\n",
    "    alpha=0.0001 # low value for denominator 0 case\n",
    "    minortiy_label=1\n",
    "    minority_indices=list(range(0,len(X)))\n",
    "#     minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    \n",
    "\n",
    "    if intra:\n",
    "        internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "        internal_distance = np.sort(internal_distance)\n",
    "        knd=internal_distance[:,num_neighbors]\n",
    "\n",
    "        knd_sorted = np.sort(knd)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # normalize it        \n",
    "    normalized_dist= (knd_sorted-np.min(knd_sorted))/(np.max(knd_sorted)-np.min(knd_sorted)+alpha)\n",
    "\n",
    "    # apply golay        \n",
    "    normalized_dist = savitzky_golay(normalized_dist, win_size, pol_order) # window size 51, polynomial order 3\n",
    "    plt.plot(normalized_dist)\n",
    "    plt.title(\"NOrmalized distance intra\"+str(intra))\n",
    "    plt.show()\n",
    "    normalized_dist=np.diff(normalized_dist)\n",
    "\n",
    "    sin_values=np.abs(np.sin(np.arctan(normalized_dist)))\n",
    "    plt.title(\"Sin differential - to get maxima intra\"+str(intra))\n",
    "    plt.plot(sin_values)\n",
    "    plt.show()\n",
    "    first_maxima_index=np.argmax(sin_values)\n",
    "    print(\"Maxima is at \",first_maxima_index)\n",
    "    proportion=first_maxima_index/sin_values.shape[0]\n",
    "    return proportion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# following function to calculate maximum\n",
    "# threshold distance\n",
    "# while placing a point\n",
    "def max_threshold_dist(X,y,num_neighbors):\n",
    "    '''\n",
    "    This function calculates the maximum distance between any two points in the minority class\n",
    "    It also calculates the minimum distance between a point in the minority and a point\n",
    "    in the majority class\n",
    "    the value returned is the minimum of the two\n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    majority_indices=[]\n",
    "    for i in range(0,y.shape[0]):\n",
    "        if i not in minority_indices:\n",
    "            majority_indices.append(i)\n",
    "    print(len(majority_indices),len(minority_indices),y.shape)\n",
    "    X_majority=X[majority_indices]\n",
    "    y_majority=y[majority_indices]\n",
    "\n",
    "\n",
    "\n",
    "    # calculate inter distance\n",
    "    internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "    internal_distance=internal_distance.flatten()\n",
    "    max_internal_distance=np.max(internal_distance)\n",
    "    \n",
    "    min_internal_distance=np.min(internal_distance[internal_distance>0])    \n",
    "\n",
    "\n",
    "\n",
    "    # additional code change\n",
    "    max_allowed_distance=min_internal_distance/max_internal_distance\n",
    "    \n",
    "    return max_allowed_distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logs/IWAE/version_0/checkpoints/last.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ngpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m chk_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_nm\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/version_0/checkpoints/last.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchk_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nm,params \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     print(nm)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     print(\"model.\"+nm in checkpoint[\"state_dict\"])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     keyy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mnm \n",
      "File \u001b[0;32m/lustre/gfxhome/asislam25/.conda/envs/prune/lib/python3.8/site-packages/torch/serialization.py:581\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    579\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/lustre/gfxhome/asislam25/.conda/envs/prune/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/lustre/gfxhome/asislam25/.conda/envs/prune/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs/IWAE/version_0/checkpoints/last.ckpt'"
     ]
    }
   ],
   "source": [
    "model_nm=\"IWAE\"\n",
    "args_filename=\"configs/iwae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "\n",
    "\n",
    "for nm,params in model.named_parameters():\n",
    "#     print(nm)\n",
    "#     print(\"model.\"+nm in checkpoint[\"state_dict\"])\n",
    "    keyy=\"model.\"+nm \n",
    "    params.data=checkpoint[\"state_dict\"][keyy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VAEDataset(**config[\"data_params\"])\n",
    "data.setup()\n",
    "tloader=data.test_dataloader()\n",
    "\n",
    "X_vals=[]\n",
    "print(\"Encoding test images...\")\n",
    "with torch.no_grad():\n",
    "    for nxt in tloader:\n",
    "#         print(len(nxt),nxt[0].shape,nxt[1].shape)\n",
    "        enc=model.encode(nxt[0])\n",
    "#         print(len(enc),enc[0].shape,enc[1].shape)\n",
    "        enc_batch=torch.cat(enc,1)\n",
    "        enc_batch=enc_batch.detach().numpy()\n",
    "        X_vals.append(enc_batch)\n",
    "        \n",
    "print(\"Completed...\")\n",
    "X_vals_arr=np.concatenate(X_vals)\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/enc\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/enc\")\n",
    "    \n",
    "np.save(\"logs/\"+model_nm+\"/enc/test_enc.npy\",X_vals_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the encoded and Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vals_arr=np.load(\"logs/\"+model_nm+\"/enc/test_enc.npy\")\n",
    "\n",
    "X=deepcopy(X_vals_arr)\n",
    "y=np.array([0 for i in range(len(X))])\n",
    "\n",
    "\n",
    "random_indices = np.random.choice(len(X), size=how_many, replace=False)\n",
    "\n",
    "X = X[random_indices, :]\n",
    "y=np.array([0 for i in range(len(X))])\n",
    "\n",
    "print(\"X=\",X.shape,\"y=\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_proportion=2\n",
    "num_neighbors=10\n",
    "n_to_sample=calculate_count_to_add(X,y,final_proportion)\n",
    "print(\"Number of new points=\",n_to_sample)\n",
    "max_dist_point=max_threshold_dist(X,y,num_neighbors)\n",
    "print(\"max_dist_point\",max_dist_point)\n",
    "proportion_intra=calculate_distance_threshold(X,y,num_neighbors,intra=True)\n",
    "proportion_minority=proportion_intra\n",
    "print(\"Proportion of population used = \",proportion_minority)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_minority=X\n",
    "y_minority=y\n",
    "\n",
    "\n",
    "\n",
    "internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "internal_distance = np.sort(internal_distance)\n",
    "knd=internal_distance[:,num_neighbors]        \n",
    "knd_sorted = np.sort(knd)        \n",
    "\n",
    "\n",
    "threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]\n",
    "print(\"Threshold distance is \",threshold_dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_cannot_use=10\n",
    "original_n_neighbors=num_neighbors\n",
    "original_max_dist_point=max_dist_point\n",
    "original_proportion=proportion_minority\n",
    "X_new_minority=[]\n",
    "N = n_to_sample\n",
    "consecutive_cannot_use=0\n",
    "while N>0:\n",
    "    for i in range(X_minority.shape[0]):\n",
    "\n",
    "        if knd[i]>threshold_dist:\n",
    "            continue\n",
    "        if N==0:\n",
    "            break\n",
    "        v = X_minority[i,:]\n",
    "        val=np.sort( abs((X_minority-v)*(X_minority-v)).sum(axis=1) )\n",
    "        # sort neighbors by distance\n",
    "        # obviously will have to ignore the \n",
    "        # first term as its a distance to iteself\n",
    "        # which wil be 0\n",
    "        posit=np.argsort(abs((X_minority-v)*(X_minority-v)).sum(axis=1))\n",
    "        kv = X_minority[posit[1:num_neighbors+1],:]\n",
    "        alphak = random.uniform(0,max_dist_point)\n",
    "        m0 = v\n",
    "#         print(m0)\n",
    "        for j in range(num_neighbors):\n",
    "#             print(kv[j,:] ,\"-\", m0)\n",
    "#             print(m0,\"+\",alphak,\"*\", (kv[j,:] - m0))\n",
    "            m1 = m0 + alphak * (kv[j,:] - m0)\n",
    "            m0 = m1\n",
    "#             print(\"res\",m0)\n",
    "        num_neighbors_to_test=math.floor(math.sqrt(num_neighbors))\n",
    "        can_use= not(check_duplicates(m0,X_minority))\n",
    "        can_use=can_use and not(check_duplicates(m0,X_new_minority))                            \n",
    "        if can_use:\n",
    "            consecutive_cannot_use=0\n",
    "            num_neighbors=min(num_neighbors+1,original_n_neighbors)\n",
    "            max_dist_point=min(max_dist_point+0.01,original_max_dist_point)\n",
    "            proportion_minority=max(proportion_minority-0.01,original_proportion)\n",
    "            threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]                \n",
    "#             print(m0)\n",
    "#             print(\"*\"*10)\n",
    "            X_new_minority.append(m0)\n",
    "            N-=1\n",
    "        else:\n",
    "            consecutive_cannot_use+=1\n",
    "            if consecutive_cannot_use>=threshold_cannot_use:\n",
    "                num_neighbors=max(num_neighbors-1,2)\n",
    "                max_dist_point=max(max_dist_point-0.01,0.01)\n",
    "                proportion_minority=min(proportion_minority+0.01,0.9)\n",
    "                threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]\n",
    "                consecutive_cannot_use=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_minority=np.array(X_new_minority)\n",
    "np.save(\"logs/\"+model_nm+\"/enc/test_aug_enc.npy\",X_new_minority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode using The VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/test_aug_enc.npy\")\n",
    "mid=X_vals_enc_arr.shape[1]//2\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:mid]\n",
    "    log_var=X_vals_enc_arr[:,mid:]\n",
    "\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    print(mu.shape,log_var.shape)\n",
    "    z = model.reparameterize(mu, log_var)    \n",
    "    images=model.decode(z)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows=5\n",
    "num_cols=5\n",
    "how_many=num_rows*num_cols\n",
    "\n",
    "random_indices = np.random.choice(len(images), size=how_many, replace=False)\n",
    "\n",
    "images_selected = images[random_indices, :]\n",
    "fig, axs = plt.subplots(num_rows,num_cols)\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axs[i][j].imshow(images[i*num_cols+j].permute(1,2,0))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "original_n_neighbors=num_neighbors\n",
    "original_max_dist_point=max_dist_point\n",
    "original_proportion=proportion_minority\n",
    "\n",
    "name=\"logs/\"+model_nm+\"/artificial/numnbrs_\"+str(original_n_neighbors)+\"_max_dist\"+str(original_max_dist_point)+\"_prop\"+str(original_proportion)\n",
    "name+=\"_sample_size\"+str(how_many)\n",
    "plt.savefig(name+\".pdf\")\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:128]\n",
    "    log_var=X_vals_enc_arr[:,128:256]\n",
    "\n",
    "    then_some=X_vals_enc_arr[:,256:]\n",
    "\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    then_some=torch.tensor(then_some)\n",
    "    z = model.reparameterize(mu, log_var)\n",
    "    z = torch.cat([z, then_some], dim = 1)\n",
    "    print(z.shape)\n",
    "    images=model.decode(z)    \n",
    "    print(images.shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/artificial/all_imgs/\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/artificial/all_imgs/\")\n",
    "\n",
    "\n",
    "    \n",
    "for img in images:\n",
    "    loc=\"logs/\"+model_nm+\"/artificial/all_imgs/\"    \n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.permute(1,2,0)\n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.detach().numpy()\n",
    "#     print(\"shp is \",img.shape,np.min(img),np.max(img))  \n",
    "    if np.min(img)<0 or np.max(img):\n",
    "        img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
    "    \n",
    "    plt.imsave(loc+\"img\"+str(counter)+\".jpeg\",img)\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "    counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
