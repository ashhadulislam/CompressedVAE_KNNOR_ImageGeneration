{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models import *\n",
    "from experiment import VAEXperiment\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from dataset import VAEDataset\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import lib_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict_classification(X,y,new_vector, num_neighbors_to_test,expected_class_index):\n",
    "    '''\n",
    "    this function is used to validate\n",
    "    whether new point generated is close to\n",
    "    same label points\n",
    "    '''\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    posit=np.argsort(abs((X-new_vector)*(X-new_vector)).sum(axis=1))\n",
    "    classes = y[posit[0:num_neighbors_to_test]]\n",
    "    return np.sum(classes==expected_class_index)==classes.shape[0]\n",
    "\n",
    "def check_duplicates( new_row,old_rows):\n",
    "    '''\n",
    "    check if the new row\n",
    "    is already preent in the old rows\n",
    "    '''\n",
    "    for row in old_rows:\n",
    "        same=True\n",
    "        for i in range(len(row)):\n",
    "            if new_row[i]!=row[i]:\n",
    "                same=False\n",
    "                continue\n",
    "        if same:\n",
    "            return True                            \n",
    "    return False\n",
    "\n",
    "def get_minority_label_index(X,y):\n",
    "    '''\n",
    "    find the minority label\n",
    "    and the indices at which minority label\n",
    "    is present\n",
    "    '''\n",
    "    # find the minority label\n",
    "    uniq_labels=np.unique(y)\n",
    "    # count for each label\n",
    "    dic_nry={}\n",
    "\n",
    "    for uniq_label in uniq_labels:\n",
    "        dic_nry[uniq_label]=0\n",
    "\n",
    "    for y_val in y:\n",
    "        dic_nry[y_val]+=1\n",
    "\n",
    "    # then which one is the minority label?\n",
    "    minority_label=-1\n",
    "    minimum_count=np.inf\n",
    "    for k,v in dic_nry.items():\n",
    "        if minimum_count>v:\n",
    "            minimum_count=v\n",
    "            minority_label=k\n",
    "\n",
    "\n",
    "    # now get the indices of the minority labels\n",
    "    minority_indices=[]\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i]==minority_label:\n",
    "            minority_indices.append(i)\n",
    "\n",
    "    return minority_label,minority_indices\n",
    "\n",
    "def good_count_neighbors(X,y):\n",
    "    '''\n",
    "    find the good number of neighbors to use\n",
    "    this function is used on auto pilot\n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    count_greater=y_minority.shape[0]\n",
    "    for i in range(X_minority.shape[0]):\n",
    "        this_point_features=X_minority[i]\n",
    "        dist = ((X_minority-this_point_features)*(X_minority-this_point_features)).sum(axis=1)\n",
    "        mean_dist=np.mean(dist)\n",
    "#         print(dist,mean_dist)\n",
    "        this_point_count_lesser = (dist < mean_dist).sum()\n",
    "        count_greater=min(this_point_count_lesser,count_greater)        \n",
    "    return count_greater\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# following function\n",
    "# to get the savitzky golay filter\n",
    "# https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter\n",
    "# https://scipy.github.io/old-wiki/pages/Cookbook/SavitzkyGolay\n",
    "# https://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-in-the-right-way\n",
    "\n",
    "def savitzky_golay(y, window_size, order, deriv=0, rate=1):         \n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "\n",
    "    try:\n",
    "        window_size = np.abs(int(window_size))\n",
    "        order = np.abs(int(order))\n",
    "    except ValueError:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with\n",
    "    # values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')\n",
    "\n",
    "\n",
    "def check_enough_minorities(X,y,num_neighbors):\n",
    "    '''\n",
    "    ideally, the total number of minority points should be\n",
    "    1 more than the total number of neighbors    \n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    if len(minority_indices)<=num_neighbors:\n",
    "        print(\"You want to use \",num_neighbors,\"neighbors, but minority data size = \",len(minority_indices))\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def calculate_count_to_add(X,y,final_proportion):\n",
    "    '''\n",
    "    Calculate the number of artificial points to be generated so that\n",
    "    (count_minority_existing+count_artificial_minority)/count_majority_existing=final_proportion\n",
    "    '''\n",
    "#     minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "#     majority_indices=[]\n",
    "#     for i in range(0,y.shape[0]):\n",
    "#         if i not in minority_indices:\n",
    "#             majority_indices.append(i)\n",
    "#     count_minority=len(minority_indices)\n",
    "#     count_majority=len(majority_indices)\n",
    "#     new_minority=int((final_proportion*count_majority)-count_minority)\n",
    "#     if new_minority<1:\n",
    "#         return -1\n",
    "    \n",
    "    \n",
    "    # extra code\n",
    "    count_to_add=int(final_proportion*len(X))\n",
    "    return count_to_add\n",
    "    \n",
    "#     return new_minority\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_distance_threshold(X,y,num_neighbors,intra=True):\n",
    "    '''\n",
    "    returns the distance threshold, based on the intra parameter\n",
    "    if intra is chosen, returns the cut-off point for distances to\n",
    "    kth nearest neighbor of same class\n",
    "    in inter is chosen, returns the cut-off point for distances to \n",
    "    kth nearest neighbor of opposite class\n",
    "\n",
    "    '''\n",
    "    win_size=5 #positive odd number\n",
    "    pol_order=2\n",
    "    alpha=0.0001 # low value for denominator 0 case\n",
    "    minortiy_label=1\n",
    "    minority_indices=list(range(0,len(X)))\n",
    "#     minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    \n",
    "\n",
    "    if intra:\n",
    "        internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "        internal_distance = np.sort(internal_distance)\n",
    "        knd=internal_distance[:,num_neighbors]\n",
    "\n",
    "        knd_sorted = np.sort(knd)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # normalize it        \n",
    "    normalized_dist= (knd_sorted-np.min(knd_sorted))/(np.max(knd_sorted)-np.min(knd_sorted)+alpha)\n",
    "\n",
    "    # apply golay        \n",
    "    normalized_dist = savitzky_golay(normalized_dist, win_size, pol_order) # window size 51, polynomial order 3\n",
    "    plt.plot(normalized_dist)\n",
    "    plt.title(\"NOrmalized distance intra\"+str(intra))\n",
    "    plt.show()\n",
    "    normalized_dist=np.diff(normalized_dist)\n",
    "\n",
    "    sin_values=np.abs(np.sin(np.arctan(normalized_dist)))\n",
    "    plt.title(\"Sin differential - to get maxima intra\"+str(intra))\n",
    "    plt.plot(sin_values)\n",
    "    plt.show()\n",
    "    first_maxima_index=np.argmax(sin_values)\n",
    "    print(\"Maxima is at \",first_maxima_index)\n",
    "    proportion=first_maxima_index/sin_values.shape[0]\n",
    "    return proportion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# following function to calculate maximum\n",
    "# threshold distance\n",
    "# while placing a point\n",
    "def max_threshold_dist(X,y,num_neighbors):\n",
    "    '''\n",
    "    This function calculates the maximum distance between any two points in the minority class\n",
    "    It also calculates the minimum distance between a point in the minority and a point\n",
    "    in the majority class\n",
    "    the value returned is the minimum of the two\n",
    "    '''\n",
    "    minority_label,minority_indices=get_minority_label_index(X,y)\n",
    "    X_minority=X[minority_indices]\n",
    "    y_minority=y[minority_indices]\n",
    "    majority_indices=[]\n",
    "    for i in range(0,y.shape[0]):\n",
    "        if i not in minority_indices:\n",
    "            majority_indices.append(i)\n",
    "    print(len(majority_indices),len(minority_indices),y.shape)\n",
    "    X_majority=X[majority_indices]\n",
    "    y_majority=y[majority_indices]\n",
    "\n",
    "\n",
    "\n",
    "    # calculate inter distance\n",
    "    internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "    internal_distance=internal_distance.flatten()\n",
    "    max_internal_distance=np.max(internal_distance)\n",
    "    \n",
    "    min_internal_distance=np.min(internal_distance[internal_distance>0])    \n",
    "\n",
    "\n",
    "\n",
    "    # additional code change\n",
    "    max_allowed_distance=min_internal_distance/max_internal_distance\n",
    "    \n",
    "    return max_allowed_distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm=\"DFCVAE\"\n",
    "args_filename=\"configs/dfc_vae.yaml\"\n",
    "with open(args_filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "model = vae_models[config['model_params']['name']](**config['model_params'])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/last.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "\n",
    "\n",
    "for nm,params in model.named_parameters():\n",
    "#     print(nm)\n",
    "#     print(\"model.\"+nm in checkpoint[\"state_dict\"])\n",
    "    keyy=\"model.\"+nm \n",
    "    params.data=checkpoint[\"state_dict\"][keyy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DFCVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Tanh()\n",
       "  )\n",
       "  (feature_network): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU(inplace=True)\n",
       "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU(inplace=True)\n",
       "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (32): ReLU(inplace=True)\n",
       "      (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (38): ReLU(inplace=True)\n",
       "      (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (42): ReLU(inplace=True)\n",
       "      (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (45): ReLU(inplace=True)\n",
       "      (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (48): ReLU(inplace=True)\n",
       "      (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (51): ReLU(inplace=True)\n",
       "      (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us prune here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/DFCVAE/version_0/checkpoints/last.ckpt\n",
      "logs/DFCVAE/version_0/checkpoints/epoch=1-step=5087.ckpt\n",
      "Generating mask\n"
     ]
    }
   ],
   "source": [
    "state_dicts=[]\n",
    "epoch_names=[\"last.ckpt\",\"epoch=1-step=5087.ckpt\"]\n",
    "\n",
    "for epoch_name in epoch_names:\n",
    "    chk_path=\"logs/\"+model_nm+\"/version_0/checkpoints/\"+epoch_name\n",
    "    print(chk_path)\n",
    "    checkpoint = torch.load(chk_path,map_location=torch.device(device))\n",
    "    state_dict=checkpoint[\"state_dict\"]\n",
    "    state_dicts.append(state_dict)\n",
    "    \n",
    "\n",
    "importance_vector=[0.8,0.2]\n",
    "evol_wts={}\n",
    "for nm,params in model.named_parameters():\n",
    "    if \"weight\" in nm and \"bn\" not in nm and \"linear\" not in nm:\n",
    "#         print(nm,params.shape)\n",
    "        keyy=\"model.\"+nm         \n",
    "#         print(state_dicts[0][keyy].shape,state_dicts[1][keyy].shape)\n",
    "#         print(state_dicts[0][keyy][0],state_dicts[1][keyy][0])        \n",
    "        new_param_values=lib_prune.get_weighted_mean(state_dicts,keyy,importance_vector)\n",
    "#         print(new_param_values[0])\n",
    "        evol_wts[nm]=new_param_values\n",
    "    \n",
    "prune_rate=0.6\n",
    "print(\"Generating mask\")\n",
    "list_mask_val=lib_prune.create_mask_from_mean_wt(model,evol_wts,prune_rate)   \n",
    "\n",
    "\n",
    "print(\"Applying mask\")\n",
    "model=lib_prune.apply_mask_model(model,list_mask_val)\n",
    "total_size,nz_size=lib_prune.model_size(model)\n",
    "compression=(total_size-nz_size)/total_size\n",
    "print(\"compression is \",compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4915161398993901"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VAEDataset(**config[\"data_params\"])\n",
    "data.setup()\n",
    "tloader=data.test_dataloader()\n",
    "\n",
    "X_vals=[]\n",
    "print(\"Encoding test images...\")\n",
    "with torch.no_grad():\n",
    "    for nxt in tloader:\n",
    "#         print(len(nxt),nxt[0].shape,nxt[1].shape)\n",
    "        enc=model.encode(nxt[0])\n",
    "#         print(len(enc),enc[0].shape,enc[1].shape)\n",
    "        enc_batch=torch.cat(enc,1)\n",
    "        enc_batch=enc_batch.detach().numpy()\n",
    "        X_vals.append(enc_batch)\n",
    "        \n",
    "print(\"Completed...\")\n",
    "X_vals_arr=np.concatenate(X_vals)\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/enc\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/enc\")\n",
    "print(\"Completed...\")\n",
    "X_vals_arr=np.concatenate(X_vals)\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/enc\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/enc\")\n",
    "    \n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/artificial\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/artificial\")    \n",
    "    \n",
    "np.save(\"logs/\"+model_nm+\"/enc/pruned_\"+str(prune_rate)+\"_\"+str(compression)+\"_test_enc.npy\",X_vals_arr)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat=model(nxt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.encode(nxt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the encoded and Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many=1000\n",
    "X_vals_arr=np.load(\"logs/\"+model_nm+\"/enc/pruned_\"+str(prune_rate)+\"_\"+str(compression)+\"_test_enc.npy\")\n",
    "print(X_vals_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X=deepcopy(X_vals_arr)\n",
    "y=np.array([0 for i in range(len(X))])\n",
    "\n",
    "\n",
    "random_indices = np.random.choice(len(X), size=how_many, replace=False)\n",
    "\n",
    "X = X[random_indices, :]\n",
    "y=np.array([0 for i in range(len(X))])\n",
    "\n",
    "print(\"X=\",X.shape,\"y=\",y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_proportion=2\n",
    "num_neighbors=10\n",
    "n_to_sample=calculate_count_to_add(X,y,final_proportion)\n",
    "print(\"Number of new points=\",n_to_sample)\n",
    "max_dist_point=max_threshold_dist(X,y,num_neighbors)\n",
    "print(\"max_dist_point\",max_dist_point)\n",
    "proportion_intra=calculate_distance_threshold(X,y,num_neighbors,intra=True)\n",
    "proportion_minority=proportion_intra\n",
    "print(\"Proportion of population used = \",proportion_minority)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_minority=X\n",
    "y_minority=y\n",
    "\n",
    "\n",
    "\n",
    "internal_distance = np.linalg.norm(X_minority - X_minority[:,None], axis = -1)\n",
    "internal_distance = np.sort(internal_distance)\n",
    "knd=internal_distance[:,num_neighbors]        \n",
    "knd_sorted = np.sort(knd)        \n",
    "\n",
    "\n",
    "threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]\n",
    "print(\"Threshold distance is \",threshold_dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_cannot_use=10\n",
    "original_n_neighbors=num_neighbors\n",
    "original_max_dist_point=max_dist_point\n",
    "original_proportion=proportion_minority\n",
    "X_new_minority=[]\n",
    "N = n_to_sample\n",
    "consecutive_cannot_use=0\n",
    "while N>0:\n",
    "    for i in range(X_minority.shape[0]):\n",
    "\n",
    "        if knd[i]>threshold_dist:\n",
    "            continue\n",
    "        if N==0:\n",
    "            break\n",
    "        v = X_minority[i,:]\n",
    "        val=np.sort( abs((X_minority-v)*(X_minority-v)).sum(axis=1) )\n",
    "        # sort neighbors by distance\n",
    "        # obviously will have to ignore the \n",
    "        # first term as its a distance to iteself\n",
    "        # which wil be 0\n",
    "        posit=np.argsort(abs((X_minority-v)*(X_minority-v)).sum(axis=1))\n",
    "        kv = X_minority[posit[1:num_neighbors+1],:]\n",
    "        alphak = random.uniform(0,max_dist_point)\n",
    "        m0 = v\n",
    "#         print(m0)\n",
    "        for j in range(num_neighbors):\n",
    "#             print(kv[j,:] ,\"-\", m0)\n",
    "#             print(m0,\"+\",alphak,\"*\", (kv[j,:] - m0))\n",
    "            m1 = m0 + alphak * (kv[j,:] - m0)\n",
    "            m0 = m1\n",
    "#             print(\"res\",m0)\n",
    "        num_neighbors_to_test=math.floor(math.sqrt(num_neighbors))\n",
    "        can_use= not(check_duplicates(m0,X_minority))\n",
    "        can_use=can_use and not(check_duplicates(m0,X_new_minority))                            \n",
    "        if can_use:\n",
    "            consecutive_cannot_use=0\n",
    "            num_neighbors=min(num_neighbors+1,original_n_neighbors)\n",
    "            max_dist_point=min(max_dist_point+0.01,original_max_dist_point)\n",
    "            proportion_minority=max(proportion_minority-0.01,original_proportion)\n",
    "            threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]                \n",
    "#             print(m0)\n",
    "#             print(\"*\"*10)\n",
    "            X_new_minority.append(m0)\n",
    "            N-=1\n",
    "        else:\n",
    "            consecutive_cannot_use+=1\n",
    "            if consecutive_cannot_use>=threshold_cannot_use:\n",
    "                num_neighbors=max(num_neighbors-1,2)\n",
    "                max_dist_point=max(max_dist_point-0.01,0.01)\n",
    "                proportion_minority=min(proportion_minority+0.01,0.9)\n",
    "                threshold_dist = knd_sorted[math.floor(proportion_minority*len(knd_sorted))]\n",
    "                consecutive_cannot_use=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_minority=np.array(X_new_minority)\n",
    "np.save(\"logs/\"+model_nm+\"/enc/pruned_\"+str(prune_rate)+\"_\"+str(compression)+\"test_aug_enc.npy\",X_new_minority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode using The VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vals_enc_arr=np.load(\"logs/\"+model_nm+\"/enc/pruned_\"+str(prune_rate)+\"_\"+str(compression)+\"test_aug_enc.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_vals_enc_arr.shape)\n",
    "mid=X_vals_enc_arr.shape[1]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:mid]\n",
    "    log_var=X_vals_enc_arr[:,mid:]\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    print(mu.shape,log_var.shape)\n",
    "    \n",
    "    z = model.reparameterize(mu, log_var)    \n",
    "    images=model.decode(z)  \n",
    "    print(images.shape)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows=5\n",
    "num_cols=5\n",
    "how_many=num_rows*num_cols\n",
    "\n",
    "random_indices = np.random.choice(len(images), size=how_many, replace=False)\n",
    "\n",
    "images_selected = images[random_indices, :]\n",
    "fig, axs = plt.subplots(num_rows,num_cols)\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axs[i][j].imshow(images[i*num_cols+j].permute(1,2,0))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "original_n_neighbors=num_neighbors\n",
    "original_max_dist_point=max_dist_point\n",
    "original_proportion=proportion_minority\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/artificial\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/artificial\")\n",
    "\n",
    "\n",
    "name=\"logs/\"+model_nm+\"/artificial/pruned\"+str(prune_rate)+\"_numnbrs_\"+str(original_n_neighbors)+\"_max_dist\"+str(original_max_dist_point)+\"_prop\"+str(original_proportion)\n",
    "name+=\"_sample_size\"+str(how_many)\n",
    "plt.savefig(name+\".pdf\")\n",
    "plt.show()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    mu=X_vals_enc_arr[:,:mid]\n",
    "    log_var=X_vals_enc_arr[:,mid:]\n",
    "    mu=torch.tensor(mu)\n",
    "    log_var=torch.tensor(log_var)\n",
    "    print(mu.shape,log_var.shape)\n",
    "    \n",
    "    z = model.reparameterize(mu, log_var)    \n",
    "    images=model.decode(z)  \n",
    "    print(images.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/artificial/all_imgs/\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/artificial/all_imgs/\")\n",
    "\n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/artificial/all_imgs/pruned\"):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/artificial/all_imgs/pruned\")\n",
    "\n",
    "    \n",
    "if not os.path.isdir(\"logs/\"+model_nm+\"/artificial/all_imgs/pruned/\"+str(prune_rate)+\"_\"+str(compression)):\n",
    "    os.mkdir(\"logs/\"+model_nm+\"/artificial/all_imgs/pruned/\"+str(prune_rate)+\"_\"+str(compression))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for img in images:\n",
    "    loc=\"logs/\"+model_nm+\"/artificial/all_imgs/pruned/\"+str(prune_rate)+\"_\"+str(compression)+\"/\"\n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.permute(1,2,0)\n",
    "#     print(\"shp is \",img.shape)\n",
    "    img=img.detach().numpy()\n",
    "#     print(\"shp is \",img.shape,np.min(img),np.max(img))  \n",
    "    if np.min(img)<0 or np.max(img):\n",
    "        img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
    "    \n",
    "    plt.imsave(loc+\"img\"+str(counter)+\".jpeg\",img)\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "    counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
